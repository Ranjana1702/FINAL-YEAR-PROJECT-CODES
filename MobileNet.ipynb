{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here, I created a MobileNetV2 style transfer pipeline from scratch, implementing the multi-resolution training schedule, optimisation steps, and unique loss functions. For stability, pretrained ImageNet weights were used to initialise the backbone extractor; nonetheless, I designed and built the pipeline on my own. In order to test MobileNet's effectiveness and guide its inclusion in the finished integrated 3-in-1 prototype, I further applied this model on a dataset to produce a portfolio of stylised outcomes."
      ],
      "metadata": {
        "id": "mPk3p9fxFKpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gdown to download the dataset folder (COCO for content + WikiArt for style images) from Google Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "# Shared Google Drive folder link\n",
        "url = \"https://drive.google.com/drive/folders/1xJy4FXcBIHKnjO5t_m8PJsevBJQ2_6G-?usp=sharing\"\n",
        "\n",
        "# Download the shared Google Drive folder into a local directory named \"dataset\"\n",
        "!gdown --folder \"$url\" -O dataset\n",
        "\n",
        "# Verify the folder contents after download\n",
        "import os\n",
        "for root, dirs, files in os.walk(\"dataset\"):\n",
        "    # Count folder depth based on subfolders inside folder\n",
        "    level = root.replace(\"dataset\", \"\").count(os.sep)\n",
        "    # Create indent spaces depending on folder depth\n",
        "    indent = \" \" * 2 * level\n",
        "    # Print the current folder name\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "    # Create indentation for files inside the folder\n",
        "    subindent = \" \" * 2 * (level + 1)\n",
        "    # Loop through each file inside the current folder and print with indentation\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")\n"
      ],
      "metadata": {
        "id": "8vGXCE1OS8ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confirm and check contents inside the downloaded folder\n",
        "import os\n",
        "\n",
        "# Define path to the dataset folder\n",
        "folder_path = \"/content/dataset\"\n",
        "\n",
        "\n",
        "# Walk through the folder and its subfolders (verify contents once again)\n",
        "for root, dirs, files in os.walk(folder_path):\n",
        "    # indentation based on depth of the folder\n",
        "    level = root.replace(folder_path, \"\").count(os.sep)\n",
        "    # Create indent spaces depending on folder depth\n",
        "    indent = \" \" * 2 * level\n",
        "    # Print the current folder name\n",
        "    print(f\"{indent}{os.path.basename(root)}/\")\n",
        "     # Create indentation for files inside the folder\n",
        "    subindent = \" \" * 2 * (level + 1)\n",
        "     # Loop through each file inside the current folder and print with indentation\n",
        "    for f in files:\n",
        "        print(f\"{subindent}{f}\")\n"
      ],
      "metadata": {
        "id": "FrPJsdXdTAYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries for deep learning and plotting\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random, time\n",
        "from tensorflow.keras.applications import mobilenet_v2\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load and resize an image for display (hi-res, keeps aspect ratio)\n",
        "def load_img_display(path, target_max_side=768):\n",
        "    # Read image from file path\n",
        "    img = tf.io.read_file(path)\n",
        "    # Decode image into 3 channels (disable animations for GIFs/webp)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    # Convert pixel values to float in [0,1]\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    # Extract original height and width of image\n",
        "    h = tf.cast(tf.shape(img)[0], tf.float32)\n",
        "    w = tf.cast(tf.shape(img)[1], tf.float32)\n",
        "    # Compute scaling factor so longest side = target_max_side\n",
        "    scale = target_max_side / tf.maximum(h, w)\n",
        "    new_h = tf.cast(tf.round(h * scale), tf.int32)\n",
        "    new_w = tf.cast(tf.round(w * scale), tf.int32)\n",
        "    img = tf.image.resize(img, (new_h, new_w), method='bicubic', antialias=True)\n",
        "    return img\n",
        "\n",
        "# Display an image using matplotlib\n",
        "def show_display(img, title=None, figsize=(7,7), dpi=150):\n",
        "    # Remove batch dimension if present\n",
        "    #so the shape becomes suitable for displaying with matplotlib\n",
        "    if len(img.shape) == 4:\n",
        "        img = tf.squeeze(img, 0)\n",
        "    # Display figure with chosen size and resolution\n",
        "    plt.figure(figsize=figsize, dpi=dpi)\n",
        "    plt.imshow(tf.clip_by_value(img, 0, 1))\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    # Remove axes for cleaner output\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Display image after preprocessing reversal (from [-1,1] to [0,1])\n",
        "def show_preprocessed(img, title=None, upscale_to=768):\n",
        "    # Remove batch dimension if present\n",
        "    if len(img.shape) == 4:\n",
        "        img = tf.squeeze(img, 0)\n",
        "    # Convert from [-1,1] → [0,1]\n",
        "    vis = tf.clip_by_value((img + 1.0)/2.0, 0, 1)\n",
        "    # Optionally upscale to uniform size for display\n",
        "    if upscale_to:\n",
        "        vis = tf.image.resize(vis, (upscale_to, upscale_to), method='bicubic', antialias=True)\n",
        "    # Display image with title\n",
        "    show_display(vis, title)\n",
        "\n",
        "# Build MobileNetV2 feature extractor given layer names\n",
        "def mbnet_layers(layer_names, img_size):\n",
        "    # Load MobileNetV2 without top classification layer, pretrained on ImageNet\n",
        "    base = mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet',\n",
        "                                    input_shape=(img_size, img_size, 3))\n",
        "    base.trainable = False\n",
        "    # Extract outputs for specified layers\n",
        "    outputs = [base.get_layer(name).output for name in layer_names]\n",
        "    return Model([base.input], outputs)\n",
        "\n",
        "# Compute Gram matrix (style representation) for feature map\n",
        "def gram_matrix(x):\n",
        "    b, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
        "    # Flatten spatial dimensions\n",
        "    feats = tf.reshape(x, [b, h*w, c])\n",
        "    # Compute inner product and normalize by number of locations\n",
        "    return tf.matmul(feats, feats, transpose_a=True) / tf.cast(h*w, tf.float32)\n",
        "\n",
        "# Model wrapper that returns both style and content features\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, img_size):\n",
        "        super().__init__()\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "        # Build encoder that outputs both style and content activations\n",
        "        self.encoder = mbnet_layers(style_layers + content_layers, img_size)\n",
        "        self.encoder.trainable = False\n",
        "    def call(self, x):\n",
        "        # Forward pass through encoder\n",
        "        outs = self.encoder(x)\n",
        "        # First outputs are style, processed via Gram matrices\n",
        "        style_outs  = [gram_matrix(o) for o in outs[:self.num_style]]\n",
        "        # Remaining outputs are content features\n",
        "        content_outs= outs[self.num_style:]\n",
        "        return {\n",
        "            'style'  : {n:v for n,v in zip(self.style_layers,  style_outs)},\n",
        "            'content': {n:v for n,v in zip(self.content_layers, content_outs)}\n",
        "        }\n",
        "\n",
        "# Total variation loss encourages smoothness in output\n",
        "def total_variation_loss(x):\n",
        "    return tf.image.total_variation(x)\n",
        "\n",
        "# Laplacian operator for edge preservation (sharpness)\n",
        "def laplacian(img_minus1_to1):\n",
        "    # Convert from [-1,1] → [0,1]\n",
        "    x = (img_minus1_to1 + 1.0)/2.0\n",
        "    # Define Laplacian kernel\n",
        "    k = tf.constant([[0.,-1.,0.],[-1.,4.,-1.],[0.,-1.,0.]], tf.float32)\n",
        "    k = tf.reshape(k, [3,3,1,1])\n",
        "    # Apply kernel across all 3 channels\n",
        "    k = tf.repeat(k, repeats=3, axis=2)\n",
        "    return tf.nn.conv2d(x, k, strides=1, padding='SAME')\n",
        "\n",
        "# Perform one optimization stage of style transfer\n",
        "def run_stage(content_path, style_path, img_size,\n",
        "              style_layers, style_wts, content_layers,\n",
        "              steps, lr_start, lr_end,\n",
        "              style_weight=3.0, content_weight=1e2, tv_weight=5e-7, edge_weight=5e-3,\n",
        "              init_from=None, preview_every=100, title_prefix=\"\"):\n",
        "\n",
        "    # Function to load and preprocess image for MobileNet\n",
        "    def load_for_model(path):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)    # [0,1]\n",
        "        img = tf.image.resize(img, (img_size, img_size), antialias=True)\n",
        "        # Preprocess to MobileNet input format ([-1,1])\n",
        "        img = mobilenet_v2.preprocess_input(img * 255.0)\n",
        "        return img[tf.newaxis, ...]\n",
        "\n",
        "    # Load content and style images\n",
        "    content = load_for_model(content_path)\n",
        "    style   = load_for_model(style_path)\n",
        "\n",
        "    # Extract target style and content features\n",
        "    extractor = StyleContentModel(style_layers, content_layers, img_size)\n",
        "    style_tgt   = extractor(style)['style']\n",
        "    content_tgt = extractor(content)['content']\n",
        "\n",
        "    # Initialize optimization variable (either blended or from previous stage)\n",
        "    if init_from is None:\n",
        "        # Blend style and content as starting point\n",
        "        style_for_blend   = tf.image.resize((style+1)/2, (img_size, img_size))\n",
        "        content_for_blend = (content+1)/2\n",
        "        alpha = 0.4\n",
        "        init = tf.clip_by_value((1-alpha)*content_for_blend + alpha*style_for_blend, 0, 1)\n",
        "        init = mobilenet_v2.preprocess_input(init * 255.0)\n",
        "        image = tf.Variable(init)\n",
        "    else:\n",
        "        # Resize previous output as initialization\n",
        "        init = tf.image.resize(init_from, (img_size, img_size), method='bicubic', antialias=True)\n",
        "        image = tf.Variable(init)\n",
        "\n",
        "    # Define cosine decay learning rate schedule\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=lr_start, decay_steps=steps, alpha=lr_end/lr_start\n",
        "    )\n",
        "    opt = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    # Normalize style weights sum\n",
        "    denom = sum(style_wts.values())\n",
        "\n",
        "    # Define single optimization step\n",
        "    @tf.function\n",
        "    def train_step(img):\n",
        "        with tf.GradientTape() as tape:\n",
        "            outs = extractor(img)\n",
        "            s, c = outs['style'], outs['content']\n",
        "            # Style loss: weighted Gram matrix differences\n",
        "            s_loss = tf.add_n([style_wts[k]*tf.reduce_mean((s[k]-style_tgt[k])**2) for k in s]) * (style_weight/denom)\n",
        "            # Content loss: MSE between features\n",
        "            c_loss = tf.add_n([tf.reduce_mean((c[k]-content_tgt[k])**2) for k in c]) * (content_weight/len(content_layers))\n",
        "            # Total variation loss\n",
        "            tv = tf.reduce_mean(total_variation_loss(img)) * tv_weight\n",
        "            # Edge-preserving loss using Laplacian\n",
        "            edge = tf.reduce_mean((laplacian(img) - laplacian(content))**2) * edge_weight\n",
        "            # Combine all losses\n",
        "            loss = s_loss + c_loss + tv + edge\n",
        "        # Compute gradients\n",
        "        grad = tape.gradient(loss, img)\n",
        "        # Clip gradients for stability\n",
        "        grad = tf.clip_by_norm(grad, 10.0)\n",
        "        # Apply gradient update\n",
        "        opt.apply_gradients([(grad, img)])\n",
        "        # Keep image within valid range\n",
        "        img.assign(tf.clip_by_value(img, -1, 1))\n",
        "        return loss\n",
        "\n",
        "    # Run iterative optimization loop\n",
        "    for step in range(steps):\n",
        "        loss = train_step(image)\n",
        "        # Periodically preview progress\n",
        "        if (step+1) % preview_every == 0:\n",
        "            show_preprocessed(image, title=f\"{title_prefix} step {step+1}\", upscale_to=min(768, img_size))\n",
        "    return image\n",
        "\n",
        "# Define MobileNetV2 style layers and their relative weights\n",
        "STYLE_LAYERS = [\n",
        "    'block_1_expand_relu','block_3_expand_relu','block_6_expand_relu',\n",
        "    'block_10_expand_relu','block_13_expand_relu'\n",
        "]\n",
        "STYLE_LAYER_WEIGHTS = {\n",
        "    'block_1_expand_relu': 1.0,\n",
        "    'block_3_expand_relu': 0.8,\n",
        "    'block_6_expand_relu': 0.6,\n",
        "    'block_10_expand_relu': 0.5,\n",
        "    'block_13_expand_relu': 0.4,\n",
        "}\n",
        "\n",
        "# Define content layers for structural preservation\n",
        "CONTENT_LAYERS = ['block_13_expand_relu', 'block_16_project']\n",
        "\n",
        "# Base weights for losses, tuned for progressive stages\n",
        "STYLE_WEIGHT   = 5.0\n",
        "#Assign content weights for Stage 1, 2 and 3\n",
        "CONTENT_WEIGHT_S1 = 1e2\n",
        "CONTENT_WEIGHT_S2 = 2e2\n",
        "CONTENT_WEIGHT_S3 = 3e2\n",
        "TV_WEIGHT      = 1e-6\n",
        "EDGE_WEIGHT    = 5e-3\n",
        "\n",
        "# Define dataset directories for content and style images\n",
        "content_dir = '/content/dataset/content'\n",
        "style_dir   = '/content/dataset/style'\n",
        "\n",
        "# Collect all valid image files from directories\n",
        "content_files = [os.path.join(content_dir, f) for f in os.listdir(content_dir)\n",
        "                 if f.lower().endswith(('.jpg','.jpeg','.png','.webp'))]\n",
        "style_files   = [os.path.join(style_dir, f) for f in os.listdir(style_dir)\n",
        "                 if f.lower().endswith(('.jpg','.jpeg','.png','.webp'))]\n",
        "\n",
        "# Ensure images exist before proceeding\n",
        "assert content_files, f\"No images found in {content_dir}\"\n",
        "assert style_files,   f\"No images found in {style_dir}\"\n",
        "\n",
        "# Randomly select one content and one style image\n",
        "content_path = random.choice(content_files)\n",
        "style_path   = random.choice(style_files)\n",
        "\n",
        "# Print chosen file names\n",
        "print(f\"\\n Content: {os.path.basename(content_path)}\")\n",
        "print(f\" Style:   {os.path.basename(style_path)}\")\n",
        "\n",
        "# Show chosen images\n",
        "show_display(load_img_display(content_path, 768), \"Content (hi-res)\")\n",
        "show_display(load_img_display(style_path, 768),   \"Style (hi-res)\")\n",
        "\n",
        "# Stage 1: Optimization at low resolution(224x224)\n",
        "print(\"\\nStage 1: 224×224\")\n",
        "out_224 = run_stage(content_path, style_path, 224,\n",
        "    STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "    steps=400, lr_start=0.05, lr_end=0.01,\n",
        "    style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S1, tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT,\n",
        "    init_from=None, preview_every=200, title_prefix=\"224x224\")\n",
        "\n",
        "# Stage 2 : Refinement at medium resolution(512x512)\n",
        "print(\"\\nStage 2: 512×512 (refine)\")\n",
        "out_512 = run_stage(content_path, style_path, 512,\n",
        "    STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "    steps=350, lr_start=0.03, lr_end=0.006,\n",
        "    style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S2, tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT,\n",
        "    init_from=out_224, preview_every=100, title_prefix=\"512x512\")\n",
        "\n",
        "# Stage 3: Sharpening at high resolution(768x768) to display clearer outputs\n",
        "print(\"\\nStage 3: 768×768 (sharpen)\")\n",
        "out_768 = run_stage(content_path, style_path, 768,\n",
        "    STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "    steps=300, lr_start=0.02, lr_end=0.004,\n",
        "    style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S3, tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT,\n",
        "    init_from=out_512, preview_every=100, title_prefix=\"768x768\")\n",
        "\n",
        "# Show final stylised output\n",
        "print(\"\\n Final output:\")\n",
        "show_preprocessed(out_768, title=\"Final Stylized Output (MobileNetV2)\", upscale_to=768)\n"
      ],
      "metadata": {
        "id": "C8sRtsguTE6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import core libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random, time\n",
        "from tensorflow.keras.applications import mobilenet_v2\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# Load and resize an image for display\n",
        "def load_img_display(path, target_size=512):\n",
        "    # Read image from file path\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    img = tf.image.resize(img, (target_size, target_size), method='bicubic', antialias=True)\n",
        "    return img\n",
        "\n",
        "# Show a 3-panel visual comparison (Content vs Style vs Stylised)\n",
        "def show_mnet_portfolio(content_img01, style_img01, stylized_minus1_to1, title=None):\n",
        "    # Convert stylized output from [-1,1] → [0,1]\n",
        "    out_disp = tf.clip_by_value((tf.squeeze(stylized_minus1_to1, 0) + 1.0)/2.0, 0, 1)\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "    # Loop through content, style, and stylized images\n",
        "    for ax, img, lab in zip(\n",
        "        axes,\n",
        "        [content_img01, style_img01, out_disp],\n",
        "        [\"Content\", \"Style\", \"Stylized\"]\n",
        "    ):\n",
        "        ax.imshow(tf.clip_by_value(img, 0, 1))\n",
        "        ax.set_title(lab)\n",
        "        #Clear axis for clearer output\n",
        "        ax.axis('off')\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Build MobileNetV2 encoder with chosen layers\n",
        "def mbnet_layers(layer_names, img_size):\n",
        "    base = mobilenet_v2.MobileNetV2(include_top=False, weights='imagenet',\n",
        "                                    input_shape=(img_size, img_size, 3))\n",
        "    base.trainable = False\n",
        "    # Extract feature maps for selected layers\n",
        "    outputs = [base.get_layer(name).output for name in layer_names]\n",
        "    return Model([base.input], outputs)\n",
        "\n",
        "# Compute Gram matrix for style representation\n",
        "def gram_matrix(x):\n",
        "    b, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
        "    # Reshape the feature map to convert pixel index to row and channel to column respectively\n",
        "    # This lets us to compute relations between channels and image\n",
        "    feats = tf.reshape(x, [b, h*w, c])\n",
        "    return tf.matmul(feats, feats, transpose_a=True) / tf.cast(h*w, tf.float32)\n",
        "\n",
        "# Extractor model for both style and content features\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, img_size):\n",
        "        super().__init__()\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "        # Build encoder over both style and content layers\n",
        "        self.encoder = mbnet_layers(style_layers + content_layers, img_size)\n",
        "        self.encoder.trainable = False\n",
        "    def call(self, x):\n",
        "        outs = self.encoder(x)\n",
        "        # Style outputs or first entries are converted to Gram matrices\n",
        "        style_outs  = [gram_matrix(o) for o in outs[:self.num_style]]\n",
        "        # The remaining entries correspond to the chosen content layers\n",
        "        content_outs= outs[self.num_style:]\n",
        "        return {\n",
        "            'style'  : {n:v for n,v in zip(self.style_layers,  style_outs)},\n",
        "            'content': {n:v for n,v in zip(self.content_layers, content_outs)}\n",
        "        }\n",
        "\n",
        "# Compute total variation loss to generate smoother images\n",
        "def total_variation_loss(x):\n",
        "    return tf.image.total_variation(x)\n",
        "\n",
        "# Laplacian filter for edge preservation\n",
        "def laplacian(img_minus1_to1):\n",
        "    x = (img_minus1_to1 + 1.0)/2.0\n",
        "    # Define Laplacian kernel\n",
        "    k = tf.constant([[0.,-1.,0.],[-1.,4.,-1.],[0.,-1.,0.]], tf.float32)\n",
        "    k = tf.reshape(k, [3,3,1,1])\n",
        "    k = tf.repeat(k, repeats=3, axis=2)\n",
        "    return tf.nn.conv2d(x, k, strides=1, padding='SAME')\n",
        "\n",
        "\n",
        "# Run one stage of iterative optimization at a chosen resolution\n",
        "def run_stage(content_path, style_path, img_size,\n",
        "              style_layers, style_wts, content_layers,\n",
        "              steps, lr_start, lr_end,\n",
        "              style_weight=5.0, content_weight=1e2, tv_weight=1e-6, edge_weight=5e-3,\n",
        "              init_from=None, preview_every=999999, title_prefix=\"\"):\n",
        "\n",
        "    # Load and preprocess image into MobileNet format\n",
        "    def load_for_model(path):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
        "        img = tf.image.convert_image_dtype(img, tf.float32)    # [0,1]\n",
        "        img = tf.image.resize(img, (img_size, img_size), antialias=True)\n",
        "        img = mobilenet_v2.preprocess_input(img * 255.0)       # [-1,1]\n",
        "        return img[tf.newaxis, ...]\n",
        "\n",
        "    # Load content and style images\n",
        "    content = load_for_model(content_path)\n",
        "    style   = load_for_model(style_path)\n",
        "\n",
        "    # Extract target style or content features\n",
        "    extractor = StyleContentModel(style_layers, content_layers, img_size)\n",
        "    style_tgt   = extractor(style)['style']\n",
        "    content_tgt = extractor(content)['content']\n",
        "\n",
        "    # Initialize optimization variable\n",
        "    if init_from is None:\n",
        "        # Blend style + content for smoother start\n",
        "        style_for_blend   = tf.image.resize((style+1)/2, (img_size, img_size))\n",
        "        content_for_blend = (content+1)/2\n",
        "        alpha = 0.4\n",
        "        init = tf.clip_by_value((1-alpha)*content_for_blend + alpha*style_for_blend, 0, 1)\n",
        "        init = mobilenet_v2.preprocess_input(init * 255.0)\n",
        "        image = tf.Variable(init)\n",
        "    else:\n",
        "        # Resize previous stage output as init\n",
        "        init = tf.image.resize(init_from, (img_size, img_size), method='bicubic', antialias=True)\n",
        "        image = tf.Variable(init)\n",
        "\n",
        "    # Cosine decay learning rate\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=lr_start, decay_steps=steps, alpha=lr_end/lr_start\n",
        "    )\n",
        "    opt = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    denom = sum(style_wts.values())\n",
        "\n",
        "    # Training step (gradient descent)\n",
        "    @tf.function\n",
        "    def train_step(img):\n",
        "        with tf.GradientTape() as tape:\n",
        "            outs = extractor(img)\n",
        "            s, c = outs['style'], outs['content']\n",
        "            # Style loss: weightedaverage of Gram matrix differences\n",
        "            s_loss = tf.add_n([style_wts[k]*tf.reduce_mean((s[k]-style_tgt[k])**2) for k in s]) * (style_weight/denom)\n",
        "            # Content loss: calculate MSE (mean squared error) between content features\n",
        "            c_loss = tf.add_n([tf.reduce_mean((c[k]-content_tgt[k])**2) for k in c]) * (content_weight/len(content_layers))\n",
        "            # Total variation loss\n",
        "            tv = tf.reduce_mean(total_variation_loss(img)) * tv_weight\n",
        "            # Edge preservation via Laplacian\n",
        "            edge = tf.reduce_mean((laplacian(img) - laplacian(content))**2) * edge_weight\n",
        "            # Combined loss\n",
        "            loss = s_loss + c_loss + tv + edge\n",
        "        # Compute gradients\n",
        "        grad = tape.gradient(loss, img)\n",
        "        # Clip gradients for stability\n",
        "        grad = tf.clip_by_norm(grad, 10.0)\n",
        "        # Apply gradient update\n",
        "        opt.apply_gradients([(grad, img)])\n",
        "        img.assign(tf.clip_by_value(img, -1, 1))\n",
        "        return loss\n",
        "\n",
        "    # Run training loop for given steps\n",
        "    for step in range(steps):\n",
        "        _ = train_step(image)\n",
        "\n",
        "    # Return final optimized image\n",
        "    return image\n",
        "\n",
        "\n",
        "# Style and content layers from MobileNet\n",
        "STYLE_LAYERS = [\n",
        "    'block_1_expand_relu','block_3_expand_relu','block_6_expand_relu',\n",
        "    'block_10_expand_relu','block_13_expand_relu'\n",
        "]\n",
        "STYLE_LAYER_WEIGHTS = {\n",
        "    'block_1_expand_relu': 1.0,\n",
        "    'block_3_expand_relu': 0.8,\n",
        "    'block_6_expand_relu': 0.6,\n",
        "    'block_10_expand_relu': 0.5,\n",
        "    'block_13_expand_relu': 0.4,\n",
        "}\n",
        "CONTENT_LAYERS = ['block_13_expand_relu', 'block_16_project']\n",
        "\n",
        "# Loss weights tuned for progressive stages\n",
        "STYLE_WEIGHT   = 5.0\n",
        "CONTENT_WEIGHT_S1 = 1e2\n",
        "CONTENT_WEIGHT_S2 = 2e2\n",
        "CONTENT_WEIGHT_S3 = 3e2\n",
        "TV_WEIGHT      = 1e-6\n",
        "EDGE_WEIGHT    = 5e-3\n",
        "\n",
        "\n",
        "# Resolve dataset paths depending on folder structure\n",
        "def resolve_dirs():\n",
        "    for c,s in [\n",
        "        (\"dataset/dataset/content\",\"dataset/dataset/style\"),\n",
        "        (\"datasets/datasets/content\",\"datasets/datasets/style\"),\n",
        "        (\"dataset/content\",\"dataset/style\"),\n",
        "    ]:\n",
        "        if os.path.isdir(c) and os.path.isdir(s):\n",
        "            return c,s\n",
        "    raise FileNotFoundError(\"Content/style folders not found.\")\n",
        "\n",
        "content_dir, style_dir = resolve_dirs()\n",
        "\n",
        "# Collect content and style images\n",
        "content_files = [os.path.join(content_dir, f) for f in os.listdir(content_dir)\n",
        "                 if f.lower().endswith(('.jpg','.jpeg','.png','.webp'))]\n",
        "style_files   = [os.path.join(style_dir, f) for f in os.listdir(style_dir)\n",
        "                 if f.lower().endswith(('.jpg','.jpeg','.png','.webp'))]\n",
        "\n",
        "# Ensure lists are not empty\n",
        "assert content_files, f\"No images in {content_dir}\"\n",
        "assert style_files,   f\"No images in {style_dir}\"\n",
        "\n",
        "# Initialise Portfolio loop\n",
        "# Number of example triplets(content|style|stylised) to generate\n",
        "NUM_SAMPLES = 6\n",
        "\n",
        "for i in range(NUM_SAMPLES):\n",
        "    # Randomly choose one content and one style from the dataset\n",
        "    content_path = random.choice(content_files)\n",
        "    style_path   = random.choice(style_files)\n",
        "    print(f\"\\nPair {i+1}: {os.path.basename(content_path)} + {os.path.basename(style_path)}\")\n",
        "\n",
        "    # Load images for display at 512px\n",
        "    content_disp = load_img_display(content_path, 512)\n",
        "    style_disp   = load_img_display(style_path,   512)\n",
        "\n",
        "    # Stage 1: low-res optimization (224×224)\n",
        "    out_224 = run_stage(content_path, style_path, 224,\n",
        "        STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "        steps=400, lr_start=0.05, lr_end=0.01,\n",
        "        style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S1,\n",
        "        tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT)\n",
        "\n",
        "    # Stage 2: refine at 512×512\n",
        "    out_512 = run_stage(content_path, style_path, 512,\n",
        "        STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "        steps=350, lr_start=0.03, lr_end=0.006,\n",
        "        style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S2,\n",
        "        tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT,\n",
        "        init_from=out_224)\n",
        "\n",
        "    # Stage 3: sharpen at 768×768\n",
        "    out_768 = run_stage(content_path, style_path, 768,\n",
        "        STYLE_LAYERS, STYLE_LAYER_WEIGHTS, CONTENT_LAYERS,\n",
        "        steps=300, lr_start=0.02, lr_end=0.004,\n",
        "        style_weight=STYLE_WEIGHT, content_weight=CONTENT_WEIGHT_S3,\n",
        "        tv_weight=TV_WEIGHT, edge_weight=EDGE_WEIGHT,\n",
        "        init_from=out_512)\n",
        "\n",
        "    # Show triplet panel (content|style|stylised)\n",
        "    show_mnet_portfolio(content_disp, style_disp, out_768, title=f\"Stylization {i+1}\")\n"
      ],
      "metadata": {
        "id": "rVhQQljwTKHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import mobilenet_v2\n",
        "from tensorflow.keras.models import Model\n",
        "from PIL import Image, ImageFilter\n",
        "import cv2\n",
        "\n",
        "# Define which MobileNetV2 layers will provide style information\n",
        "STYLE_LAYERS = [\n",
        "    'block_1_expand_relu','block_3_expand_relu','block_6_expand_relu',\n",
        "    'block_10_expand_relu','block_13_expand_relu'\n",
        "]\n",
        "\n",
        "# Assign relative importance (weights) to each style layer\n",
        "STYLE_LAYER_WEIGHTS = {\n",
        "    'block_1_expand_relu': 1.0,\n",
        "    'block_3_expand_relu': 0.8,\n",
        "    'block_6_expand_relu': 0.6,\n",
        "    'block_10_expand_relu': 0.5,\n",
        "    'block_13_expand_relu': 0.4,\n",
        "}\n",
        "\n",
        "# Use deeper layers for content since they capture stronger semantic structure\n",
        "CONTENT_LAYERS = ['block_13_expand_relu', 'block_16_project']\n",
        "\n",
        "\n",
        "# Build a MobileNetV2 encoder that outputs chosen feature maps\n",
        "def mbnet_layers(layer_names, img_size):\n",
        "    # Load pretrained MobileNetV2 without classifier head\n",
        "    base = mobilenet_v2.MobileNetV2(\n",
        "        include_top=False, weights='imagenet', input_shape=(img_size, img_size, 3)\n",
        "    )\n",
        "    # Freeze parameters to avoid training\n",
        "    base.trainable = False\n",
        "    # Select intermediate layers that will serve as style/content features\n",
        "    outputs = [base.get_layer(name).output for name in layer_names]\n",
        "    return Model([base.input], outputs)\n",
        "\n",
        "# Compute Gram matrix, which encodes correlations between feature maps (style patterns)\n",
        "def gram_matrix(x):\n",
        "    b, h, w, c = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
        "    # Flatten spatial dimensions: each row = one pixel location, columns = channels\n",
        "    feats = tf.reshape(x, [b, h*w, c])\n",
        "    # Multiply with transpose to compute channel-to-channel correlations\n",
        "    return tf.matmul(feats, feats, transpose_a=True) / tf.cast(h*w, tf.float32)\n",
        "\n",
        "# Model wrapper to return both style and content features\n",
        "class StyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, img_size):\n",
        "        super().__init__()\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "        # Create encoder with all required layers\n",
        "        self.encoder = mbnet_layers(style_layers + content_layers, img_size)\n",
        "        self.encoder.trainable = False\n",
        "    def call(self, x):\n",
        "        # Forward pass to get feature maps\n",
        "        outs = self.encoder(x)\n",
        "        # Convert style features into Gram matrices\n",
        "        style_outs  = [gram_matrix(o) for o in outs[:self.num_style]]\n",
        "        # Remaining outputs correspond to chosen content layers\n",
        "        content_outs= outs[self.num_style:]\n",
        "        return {\n",
        "            'style'  : {n:v for n,v in zip(self.style_layers,  style_outs)},\n",
        "            'content': {n:v for n,v in zip(self.content_layers, content_outs)}\n",
        "        }\n",
        "\n",
        "# Regularization loss to encourage smoothness in final image\n",
        "def total_variation_loss(x):\n",
        "    return tf.image.total_variation(x)\n",
        "\n",
        "# Laplacian filter to preserve edges from content image\n",
        "def laplacian(img_minus1_to1):\n",
        "    # Convert [-1,1] image back to [0,1]\n",
        "    x = (img_minus1_to1 + 1.0)/2.0\n",
        "    # Define Laplacian kernel to detect edges\n",
        "    k = tf.constant([[0.,-1.,0.],[-1.,4.,-1.],[0.,-1.,0.]], tf.float32)\n",
        "    k = tf.reshape(k, [3,3,1,1])\n",
        "    # Apply same kernel across RGB channels\n",
        "    k = tf.repeat(k, 3, axis=2)\n",
        "    return tf.nn.conv2d(x, k, strides=1, padding='SAME')\n",
        "\n",
        "\n",
        "# Convert PIL image to MobileNetV2 preprocessed tensor\n",
        "def preprocess_for_model(pil_img, img_size):\n",
        "    # Convert to numpy and normalize to [0,1]\n",
        "    x = np.array(pil_img).astype(np.float32) / 255.0\n",
        "    # Resize to target resolution\n",
        "    x = tf.image.resize(x, (img_size, img_size), antialias=True)\n",
        "    # Apply MobileNet preprocessing (scale to [-1,1])\n",
        "    x = mobilenet_v2.preprocess_input(x * 255.0)\n",
        "    return x[tf.newaxis, ...]\n",
        "\n",
        "# Convert model tensor back to displayable image [0,1]\n",
        "def deprocess_from_model(x_minus1_to1):\n",
        "    x = tf.squeeze(x_minus1_to1, 0)  # remove batch dimension\n",
        "    x = tf.clip_by_value((x + 1.0)/2.0, 0.0, 1.0)\n",
        "    return x.numpy().astype(np.float32)\n",
        "\n",
        "\n",
        "# Gather style images and normalize their blending weights\n",
        "def collect_styles_and_weights(styles, weights):\n",
        "    pairs = [(img, w) for img, w in zip(styles, weights) if img is not None]\n",
        "    if not pairs:\n",
        "        raise ValueError(\"At least one style image must be provided.\")\n",
        "    imgs, ws = zip(*pairs)\n",
        "    total = float(sum(ws))\n",
        "    # Normalize so weights sum to 1\n",
        "    if total <= 1e-8:\n",
        "        ws = [1.0] + [0.0]*(len(imgs)-1)\n",
        "    else:\n",
        "        ws = [float(w)/total for w in ws]\n",
        "    return list(imgs), ws\n",
        "\n",
        "# Blend multiple style targets by weighted averaging Gram matrices\n",
        "def blended_style_targets(extractor, style_pils, blend_ws, img_size):\n",
        "    accum = None\n",
        "    for img, w in zip(style_pils, blend_ws):\n",
        "        st = extractor(preprocess_for_model(img, img_size))['style']\n",
        "        if accum is None:\n",
        "            accum = {k: v * w for k, v in st.items()}\n",
        "        else:\n",
        "            for k in accum:\n",
        "                accum[k] += st[k] * w\n",
        "    return accum\n",
        "\n",
        "\n",
        "# Run one stage of optimization at a given resolution\n",
        "def run_stage(content_pil, style_pils, blend_ws, img_size,\n",
        "              content_layers, style_layers, style_layer_weights,\n",
        "              steps, lr_start, lr_end,\n",
        "              alpha_content, beta_style, tv_weight=1e-6, edge_weight=5e-3,\n",
        "              init_from=None):\n",
        "\n",
        "    # Preprocess content image\n",
        "    content = preprocess_for_model(content_pil, img_size)\n",
        "    # Extractor for style and content\n",
        "    extractor = StyleContentModel(style_layers, content_layers, img_size)\n",
        "    style_tgt   = blended_style_targets(extractor, style_pils, blend_ws, img_size)\n",
        "    content_tgt = extractor(content)['content']\n",
        "\n",
        "    # Initialize optimization image\n",
        "    if init_from is None:\n",
        "        # Blend content and style seed for smoother start\n",
        "        style_seed = preprocess_for_model(style_pils[0], img_size)\n",
        "        seed = tf.clip_by_value(((content+1)/2.0)*0.6 + ((style_seed+1)/2.0)*0.4, 0, 1)\n",
        "        image = tf.Variable(mobilenet_v2.preprocess_input(seed * 255.0))\n",
        "    else:\n",
        "        # Upsample from previous stage output\n",
        "        up = tf.image.resize(init_from, (img_size, img_size), method='bicubic', antialias=True)\n",
        "        image = tf.Variable(up)\n",
        "\n",
        "    # Learning rate schedule for stability\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "        initial_learning_rate=lr_start, decay_steps=steps, alpha=lr_end/lr_start\n",
        "    )\n",
        "    opt = tf.optimizers.Adam(learning_rate=lr_schedule)\n",
        "    denom = sum(style_layer_weights.values())\n",
        "\n",
        "    # Training step computing all losses\n",
        "    @tf.function\n",
        "    def train_step(img):\n",
        "        with tf.GradientTape() as tape:\n",
        "            outs = extractor(img)\n",
        "            s, c = outs['style'], outs['content']\n",
        "            # Style loss: Gram differences weighted by layer importance\n",
        "            s_loss = tf.add_n([style_layer_weights[k]*tf.reduce_mean((s[k]-style_tgt[k])**2) for k in s]) * (beta_style/denom)\n",
        "            # Content loss: mean squared error between features\n",
        "            c_loss = tf.add_n([tf.reduce_mean((c[k]-content_tgt[k])**2) for k in c]) * (alpha_content/len(content_layers))\n",
        "            # Total variation for smoothness\n",
        "            tv = tf.reduce_mean(total_variation_loss(img)) * tv_weight\n",
        "            # Laplacian to preserve edges\n",
        "            edge = tf.reduce_mean((laplacian(img) - laplacian(content))**2) * edge_weight\n",
        "            # Total loss\n",
        "            loss = s_loss + c_loss + tv + edge\n",
        "        # Compute gradients\n",
        "        grad = tape.gradient(loss, img)\n",
        "        # Clip gradients to avoid exploding updates\n",
        "        grad = tf.clip_by_norm(grad, 10.0)\n",
        "        # Apply updates\n",
        "        opt.apply_gradients([(grad, img)])\n",
        "        # Clamp pixel values to valid range\n",
        "        img.assign(tf.clip_by_value(img, -1.0, 1.0))\n",
        "        return loss\n",
        "\n",
        "    # Run multiple optimization steps\n",
        "    for _ in range(steps):\n",
        "        _ = train_step(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "# Convert numpy array to float [0,1]\n",
        "def to_float01(np_img):\n",
        "    if np_img.dtype == np.uint8:\n",
        "        arr = np_img.astype(np.float32) / 255.0\n",
        "    else:\n",
        "        arr = np_img.astype(np.float32)\n",
        "    return np.clip(arr, 0.0, 1.0)\n",
        "\n",
        "# Create binary mask where black pixels = foreground region\n",
        "def make_black_mask_from_np(content_np_resized, threshold=0.05):\n",
        "    mask = np.all(content_np_resized < threshold, axis=-1).astype(np.float32)\n",
        "    return mask[..., None]\n",
        "\n",
        "# Preserve original colors by transferring only luminance channel\n",
        "def apply_colour_preservation(stylized_f01, content_resized_f01):\n",
        "    # OpenCV expects 8-bit inputs for colour conversion\n",
        "    # So, Convert both stylised and content images to uint8 [0,255]\n",
        "    stylized_u8 = (np.clip(stylized_f01, 0, 1) * 255).astype(np.uint8)\n",
        "    content_u8  = (np.clip(content_resized_f01, 0, 1) * 255).astype(np.uint8)\n",
        "    # Convert both style and content images from RGB to YUV colour space\n",
        "    stylized_yuv = cv2.cvtColor(stylized_u8, cv2.COLOR_RGB2YUV)\n",
        "    content_yuv  = cv2.cvtColor(content_u8,  cv2.COLOR_RGB2YUV)\n",
        "    # Replace content’s luminance channel with stylized luminance channel to keep content colour\n",
        "    combined_yuv = content_yuv.copy()\n",
        "    combined_yuv[..., 0] = stylized_yuv[..., 0]\n",
        "    # Convert the final YUV image back to RGB\n",
        "    final_rgb = cv2.cvtColor(combined_yuv, cv2.COLOR_YUV2RGB)\n",
        "    return final_rgb.astype(np.float32) / 255.0\n",
        "\n",
        "# Apply sharpening using UnsharpMask filter\n",
        "def apply_sharpness(stylized_f01, sharpness_value):\n",
        "    img_u8 = (np.clip(stylized_f01, 0, 1) * 255).astype(np.uint8)\n",
        "    pil_img = Image.fromarray(img_u8)\n",
        "    pil_img = pil_img.filter(ImageFilter.UnsharpMask(radius=1.5, percent=int(sharpness_value * 200)))\n",
        "    return np.asarray(pil_img).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Main Gradio callback function\n",
        "def stylize_ui(content,\n",
        "               s1, s2, s3, s4, s5,\n",
        "               w1, w2, w3, w4, w5,\n",
        "               alpha, beta, preserve_colour, fg_only, sharpness):\n",
        "    # Collect style images and normalize weights\n",
        "    style_imgs, blend_ws = collect_styles_and_weights(\n",
        "        [s1, s2, s3, s4, s5], [w1, w2, w3, w4, w5]\n",
        "    )\n",
        "\n",
        "    # Stage 1 optimization (low resolution 224x224)\n",
        "    out_224 = run_stage(content, style_imgs, blend_ws, 224,\n",
        "        CONTENT_LAYERS, STYLE_LAYERS, STYLE_LAYER_WEIGHTS,\n",
        "        steps=400, lr_start=0.05, lr_end=0.01,\n",
        "        alpha_content=alpha, beta_style=beta)\n",
        "\n",
        "    # Stage 2 optimization (refine at 512x512)\n",
        "    out_512 = run_stage(content, style_imgs, blend_ws, 512,\n",
        "        CONTENT_LAYERS, STYLE_LAYERS, STYLE_LAYER_WEIGHTS,\n",
        "        steps=350, lr_start=0.03, lr_end=0.006,\n",
        "        alpha_content=alpha*2.0, beta_style=beta,\n",
        "        init_from=out_224)\n",
        "\n",
        "    # Stage 3 optimization (sharpen at 768x768) to give clearer stylised outputs\n",
        "    out_768 = run_stage(content, style_imgs, blend_ws, 768,\n",
        "        CONTENT_LAYERS, STYLE_LAYERS, STYLE_LAYER_WEIGHTS,\n",
        "        steps=300, lr_start=0.02, lr_end=0.004,\n",
        "        alpha_content=alpha*3.0, beta_style=beta,\n",
        "        init_from=out_512)\n",
        "\n",
        "    # Convert final output back to [0,1] and resize output to 512 for display\n",
        "    stylized = deprocess_from_model(out_768)\n",
        "    stylized = tf.image.resize(stylized, (512, 512), antialias=True, method='bicubic').numpy()\n",
        "\n",
        "    # Prepare resized content for optional colour or foreground operations\n",
        "    content_np = to_float01(np.array(content))\n",
        "    content_resized = np.array(\n",
        "        Image.fromarray((content_np * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)\n",
        "    ).astype(np.float32) / 255.0\n",
        "\n",
        "    # Apply optional colour preservation\n",
        "    if preserve_colour:\n",
        "        stylized = apply_colour_preservation(stylized, content_resized)\n",
        "\n",
        "    # Apply optional sharpness\n",
        "    if sharpness and sharpness > 0:\n",
        "        stylized = apply_sharpness(stylized, sharpness)\n",
        "\n",
        "    # Apply foreground-only masking\n",
        "    if fg_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        stylized = mask * stylized + (1.0 - mask) * content_resized\n",
        "\n",
        "    return np.clip(stylized, 0.0, 1.0)\n",
        "\n",
        "\n",
        "# Gradio UI definition\n",
        "with gr.Blocks(css=\"\"\"\n",
        "  body { background-color: #fafafa; font-family: 'Helvetica Neue', sans-serif; }\n",
        "  h1, h2 { color: #2a4d8f; }\n",
        "  .gr-button { background: #2a4d8f !important; color: #ffffff !important; border-radius: 10px !important; }\n",
        "  .gr-button:hover { background: #3c6bd9 !important; }\n",
        "  .gr-image { border-radius: 14px; box-shadow: 0 5px 12px rgba(0,0,0,0.12); }\n",
        "\"\"\") as demo:\n",
        "\n",
        "    # App title and description\n",
        "    gr.Markdown(\"## MobileNetNST Multi-Style Transfer App — MobileNetV2\")\n",
        "    gr.Markdown(\"Upload a content image and up to five style images. Adjust blending weights, α/β, color preservation, sharpness, and foreground-only styling. Runs MobileNetV2 NST with Gram matrices, TV loss, Laplacian edge term, and a 3-stage coarse→fine schedule.\")\n",
        "\n",
        "    # Content image input\n",
        "    with gr.Row():\n",
        "        content_input = gr.Image(label=\"Content Image\", image_mode=\"RGB\", height=256, width=256, type=\"pil\")\n",
        "\n",
        "    # Style image inputs\n",
        "    with gr.Row():\n",
        "        style_input1  = gr.Image(label=\"Style 1\", image_mode=\"RGB\", height=180, width=180, type=\"pil\")\n",
        "        style_input2  = gr.Image(label=\"Style 2 (Optional)\", image_mode=\"RGB\", height=180, width=180, type=\"pil\")\n",
        "        style_input3  = gr.Image(label=\"Style 3 (Optional)\", image_mode=\"RGB\", height=180, width=180, type=\"pil\")\n",
        "        style_input4  = gr.Image(label=\"Style 4 (Optional)\", image_mode=\"RGB\", height=180, width=180, type=\"pil\")\n",
        "        style_input5  = gr.Image(label=\"Style 5 (Optional)\", image_mode=\"RGB\", height=180, width=180, type=\"pil\")\n",
        "\n",
        "    # Sliders for style blending strengths\n",
        "    with gr.Row():\n",
        "        blend1 = gr.Slider(0.0, 1.0, value=1.0, step=0.01, label=\"Style 1 Strength\")\n",
        "        blend2 = gr.Slider(0.0, 1.0, value=0.0, step=0.01, label=\"Style 2 Strength\")\n",
        "        blend3 = gr.Slider(0.0, 1.0, value=0.0, step=0.01, label=\"Style 3 Strength\")\n",
        "        blend4 = gr.Slider(0.0, 1.0, value=0.0, step=0.01, label=\"Style 4 Strength\")\n",
        "        blend5 = gr.Slider(0.0, 1.0, value=0.0, step=0.01, label=\"Style 5 Strength\")\n",
        "\n",
        "    #  Sliders and checkboxes for hyperparameters\n",
        "    with gr.Row():\n",
        "        alpha_slider = gr.Slider(minimum=1e3, maximum=1e5, value=1e4, step=100, label=\"α (Content Weight)\")\n",
        "        beta_slider  = gr.Slider(minimum=1e-3, maximum=1.0, value=1e-2, step=1e-3, label=\"β (Style Weight)\")\n",
        "        preserve_colour  = gr.Checkbox(label=\"Apply Colour Prservation\", value=False)\n",
        "        fg_only         = gr.Checkbox(label=\"Apply Foreground Styling\", value=False)\n",
        "        sharpness       = gr.Slider(minimum=0.0, maximum=1.0, value=0.5, step=0.05, label=\"Style Sharpness\")\n",
        "\n",
        "    # Stylise button and output image\n",
        "    run_button   = gr.Button(\"Stylise\")\n",
        "    output_image = gr.Image(label=\"Stylised Output\", image_mode=\"RGB\", height=512, width=512)\n",
        "\n",
        "    # Connect button to callback\n",
        "    run_button.click(\n",
        "        fn=stylize_ui,\n",
        "        inputs=[\n",
        "            content_input,\n",
        "            style_input1, style_input2, style_input3, style_input4, style_input5,\n",
        "            blend1, blend2, blend3, blend4, blend5,\n",
        "            alpha_slider, beta_slider, preserve_colour, fg_only, sharpness\n",
        "        ],\n",
        "        outputs=output_image\n",
        "    )\n",
        "\n",
        "    gr.Markdown(\"Built with TensorFlow and Gradio for MobileNet NST \")\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "yOcGoWsKSz8c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}