{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For AdaIN, I integrated the pretrained Magenta TensorFlow Hub model as the encoder–decoder backbone, since training AdaIN from scratch was outside scope given its complexity. My contribution was to extend and adapt the pretrained pipeline, adding multi-style blending, colour preservation, sharpness control, and foreground masking. These modifications transformed AdaIN into a flexible, interactive system and ensured it could be fairly compared with the scratch-built VGG19 and MobileNetV2 pipelines within the final 3-in-1 prototype."
      ],
      "metadata": {
        "id": "9vqwP3twF9UI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T2bqCN_c1J-Y"
      },
      "outputs": [],
      "source": [
        "# Install required libraries for deep learning operations\n",
        "!pip install -q tensorflow tensorflow_hub gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow for deep learning operations\n",
        "import tensorflow as tf\n",
        "# TensorFlow Hub to load pretrained models such as AdaIN\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "# Gradio to build a simple user interface\n",
        "import gradio as gr\n",
        "from PIL import Image\n"
      ],
      "metadata": {
        "id": "FYmV5t6d1TIC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load an uploaded image and prepare it for a model\n",
        "def load_image_from_upload(image, target_size=(256, 256)):\n",
        "\n",
        "    # Convert the uploaded image to RGB to ensure there are 3 channels\n",
        "    image = image.convert(\"RGB\").resize(target_size)\n",
        "    # Turn the PIL image into a NumPy array and scale pixel values to [0,1]\n",
        "    img = np.array(image).astype(np.float32) / 255.0\n",
        "    # Add a batch dimension so the shape becomes [1, height, width, channels]\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    return tf.convert_to_tensor(img, dtype=tf.float32)\n"
      ],
      "metadata": {
        "id": "pLrqew0E1bCI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to apply style transfer on a content image with a given style image\n",
        "def stylize_image(content_img, style_img):\n",
        "    try:\n",
        "        # If the content image or style image is a NumPy array, convert it back to a PIL image\n",
        "        if isinstance(content_img, np.ndarray):\n",
        "            content_img = Image.fromarray(content_img.astype(\"uint8\"))\n",
        "        if isinstance(style_img, np.ndarray):\n",
        "            style_img = Image.fromarray(style_img.astype(\"uint8\"))\n",
        "        content_tensor = load_image_from_upload(content_img)\n",
        "\n",
        "        # Preprocess the style image into a model-ready tensor\n",
        "        style_tensor = load_image_from_upload(style_img)\n",
        "        # Run the style transfer model on the content and style tensors\n",
        "        stylized_image = model(content_tensor, style_tensor)[0]\n",
        "        # Remove the batch dimension and convert to NumPy array\n",
        "        output = tf.squeeze(stylized_image).numpy()\n",
        "        # Rescale pixel values\n",
        "        output = np.clip(output * 255, 0, 255).astype('uint8')\n",
        "        # Convert the NumPy array back into a PIL image for display/output\n",
        "        return Image.fromarray(output)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\" ERROR:\", e)\n",
        "        return Image.new('RGB', (256, 256), color='red')\n"
      ],
      "metadata": {
        "id": "7sGM8BrO1imj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pretrained AdaIN style transfer model from TensorFlow Hub\n",
        "model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "\n",
        "# Function to blend multiple styles using AdaIN with weighted alphas\n",
        "def blend_styles_with_adain(content_img, style_imgs, alphas):\n",
        "    # Preprocess style image to tensors\n",
        "    style_tensors = [\n",
        "        load_image_from_upload(Image.fromarray(img.astype(\"uint8\")) if isinstance(img, np.ndarray) else img)\n",
        "        for img in style_imgs\n",
        "    ]\n",
        "\n",
        "    # Normalize style weights\n",
        "    alphas = np.array(alphas)\n",
        "    alphas = alphas / alphas.sum()\n",
        "\n",
        "    # Start with a zero tensor and add each style weighted by its alpha\n",
        "    blended_style = tf.zeros_like(style_tensors[0])\n",
        "    for i in range(len(style_tensors)):\n",
        "        blended_style += style_tensors[i] * alphas[i]\n",
        "\n",
        "    # Preprocess the content image (NumPy → PIL if needed, then to tensor)\n",
        "    content_tensor = load_image_from_upload(\n",
        "        Image.fromarray(content_img.astype(\"uint8\")) if isinstance(content_img, np.ndarray) else content_img\n",
        "    )\n",
        "\n",
        "    # Run style transfer model with the blended style tensor\n",
        "    stylized = model(content_tensor, blended_style)[0]\n",
        "    output = tf.squeeze(stylized).numpy()\n",
        "    output = np.clip(output * 255, 0, 255).astype('uint8')\n",
        "\n",
        "    # Convert NumPy array back into a PIL image\n",
        "    return Image.fromarray(output)\n"
      ],
      "metadata": {
        "id": "_cKwAOT51ncd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the OpenCV library (cv2) for image processing functions\n",
        "!pip install opencv-python\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0YaZ-8x1tD7",
        "outputId": "770160e2-964d-4a20-eadf-5b7f9b0c1932"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the OpenCV library\n",
        "import cv2\n",
        "# Print the installed OpenCV version to confirm successful installation\n",
        "print(cv2.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZN6649R1wAl",
        "outputId": "aa53a046-c3a7-4a26-a991-e2d6f33e2eb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "# Only applies style to those black pixels\n",
        "def make_black_mask_from_np(pil_img, threshold=30):\n",
        "    np_img = np.array(pil_img)\n",
        "    lower = np.array([0, 0, 0])\n",
        "    upper = np.array([threshold, threshold, threshold])\n",
        "    mask = cv2.inRange(np_img, lower, upper)\n",
        "    return mask // 255  # binary mask 0/1\n",
        "\n",
        "\n",
        "# Apply style only to foreground if white background is\n",
        "def apply_fg_aware_style(content_pil, stylized_pil, fg_checkbox_enabled=True):\n",
        "    if fg_checkbox_enabled:\n",
        "        mask = make_black_mask_from_np(content_pil, threshold=30)\n",
        "        mask_3ch = np.stack([mask]*3, axis=-1)\n",
        "        content_np = np.array(content_pil.convert(\"RGB\"))\n",
        "        stylized_np = np.array(stylized_pil.convert(\"RGB\"))\n",
        "        blended = np.where(mask_3ch == 1, stylized_np, content_np)\n",
        "        return Image.fromarray(blended.astype(np.uint8))\n",
        "    else:\n",
        "        return stylized_pil\n",
        "\n",
        "\n",
        "# Your existing style blending logic will be called here\n",
        "def process(content_img, style1, style2, style3, style4, style5,\n",
        "            s1_strength, s2_strength, s3_strength, s4_strength, s5_strength,\n",
        "            sharpness_value, preserve_colour, fg_checkbox):\n",
        "\n",
        "    style_imgs = [style1, style2, style3, style4, style5]\n",
        "    style_strengths = [s1_strength, s2_strength, s3_strength, s4_strength, s5_strength]\n",
        "\n",
        "    valid_imgs = []\n",
        "    valid_weights = []\n",
        "\n",
        "    for img, w in zip(style_imgs, style_strengths):\n",
        "        if img is not None and w > 0:\n",
        "            valid_imgs.append(img)\n",
        "            valid_weights.append(w)\n",
        "\n",
        "    if len(valid_imgs) == 0:\n",
        "        raise gr.Error(\"At least one style image with non-zero strength is required.\")\n",
        "\n",
        "    total = sum(valid_weights)\n",
        "    valid_weights = [w / total for w in valid_weights]\n",
        "\n",
        "    content_img = Image.fromarray(content_img) if isinstance(content_img, np.ndarray) else content_img\n",
        "    content_img_resized = content_img.resize((256, 256))\n",
        "\n",
        "    #Call blending function for AdaIN\n",
        "    output_pil = blend_styles_with_adain(content_img_resized, valid_imgs, valid_weights)\n",
        "\n",
        "    # Preserve original colors if requested\n",
        "    if preserve_colour:\n",
        "        try:\n",
        "            stylized_np = np.array(output_pil.convert(\"RGB\")).astype(np.uint8)\n",
        "            content_np = np.array(content_img_resized.convert(\"RGB\")).astype(np.uint8)\n",
        "            stylized_yuv = cv2.cvtColor(stylized_np, cv2.COLOR_RGB2YUV)\n",
        "            content_yuv = cv2.cvtColor(content_np, cv2.COLOR_RGB2YUV)\n",
        "            combined_yuv = content_yuv.copy()\n",
        "            combined_yuv[..., 0] = stylized_yuv[..., 0]\n",
        "            final_rgb = cv2.cvtColor(combined_yuv, cv2.COLOR_YUV2RGB)\n",
        "            output_pil = Image.fromarray(final_rgb)\n",
        "        except Exception as e:\n",
        "            raise gr.Error(f\"Colour preservation failed: {e}\")\n",
        "\n",
        "    # Apply sharpness filter\n",
        "    output_pil = output_pil.filter(ImageFilter.UnsharpMask(radius=2, percent=int(sharpness_value * 200)))\n",
        "\n",
        "    # Apply foreground-aware blending\n",
        "    output_pil = apply_fg_aware_style(content_img_resized, output_pil, fg_checkbox)\n",
        "\n",
        "    return output_pil\n",
        "\n",
        "#  GRADIO UI definition\n",
        "with gr.Blocks() as demo:\n",
        "    # App title and description\n",
        "    gr.Markdown(\"##  AdaIN Multi-Style Transfer App\")\n",
        "    gr.Markdown(\"Upload one content image and up to five style images. Control style influence, sharpness, and optional foreground-aware styling.\")\n",
        "    # Content image input\n",
        "    with gr.Row():\n",
        "        content_input = gr.Image(label=\" Content Image\", image_mode=\"RGB\", height=256, width=256)\n",
        "    # Style image inputs\n",
        "    with gr.Row():\n",
        "        style_input1 = gr.Image(label=\" Style Image 1 (Required)\", image_mode=\"RGB\", height=256, width=256)\n",
        "        style_input2 = gr.Image(label=\" Style Image 2 (Optional)\", image_mode=\"RGB\", height=256, width=256)\n",
        "        style_input3 = gr.Image(label=\" Style Image 3 (Optional)\", image_mode=\"RGB\", height=256, width=256)\n",
        "        style_input4 = gr.Image(label=\" Style Image 4 (Optional)\", image_mode=\"RGB\", height=256, width=256)\n",
        "        style_input5 = gr.Image(label=\" Style Image 5 (Optional)\", image_mode=\"RGB\", height=256, width=256)\n",
        "    # Sliders for style blending strengths\n",
        "    with gr.Row():\n",
        "        slider1 = gr.Slider(minimum=0.0, maximum=1.0, value=1.0, label=\"Style 1 Strength\")\n",
        "        slider2 = gr.Slider(minimum=0.0, maximum=1.0, value=0.0, label=\"Style 2 Strength\")\n",
        "        slider3 = gr.Slider(minimum=0.0, maximum=1.0, value=0.0, label=\"Style 3 Strength\")\n",
        "        slider4 = gr.Slider(minimum=0.0, maximum=1.0, value=0.0, label=\"Style 4 Strength\")\n",
        "        slider5 = gr.Slider(minimum=0.0, maximum=1.0, value=0.0, label=\"Style 5 Strength\")\n",
        "    #  Sliders and checkboxes for hyperparameters\n",
        "    with gr.Row():\n",
        "        sharpness_slider = gr.Slider(minimum=0.0, maximum=1.0, value=0.5, label=\" Style Sharpness\")\n",
        "\n",
        "    with gr.Row():\n",
        "        preserve_colour = gr.Checkbox(label=\"Apply Colour Preservation\", value=False)\n",
        "        fg_checkbox = gr.Checkbox(label=\"Apply Foreground-Aware Styling\", value=False)\n",
        "    # Stylise button and output image\n",
        "    run_button = gr.Button(\" Stylise\")\n",
        "    output_image = gr.Image(label=\" Stylised Output\", image_mode=\"RGB\", height=256, width=256)\n",
        "\n",
        "    # Connect button to callback\n",
        "    run_button.click(\n",
        "        fn=process,\n",
        "        inputs=[\n",
        "            content_input,\n",
        "            style_input1, style_input2, style_input3, style_input4, style_input5,\n",
        "            slider1, slider2, slider3, slider4, slider5,\n",
        "            sharpness_slider,\n",
        "            preserve_colour,\n",
        "            fg_checkbox\n",
        "        ],\n",
        "        outputs=output_image\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "q9Q-MxGQJHQq",
        "outputId": "a3eb3df0-6700-4422-dab1-c09305e5f527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://1ee812080ca2b21e6b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1ee812080ca2b21e6b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxR6L6uth4_H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}