{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "All the effort invested in developing and testing the individual pipelines—ranging from the scratch-built VGG19 and MobileNet optimizers to the integration of the AdaIN approach—was ultimately consolidated into a unified Gradio interface. This 3-in-1 application provides a streamlined, user-friendly platform where each model can be selected, styles blended, and features such as colour preservation, foreground masking, and sharpness control applied. Bringing these pipelines together in a single interface not only highlights the robustness of each approach but also demonstrates how they complement one another within a cohesive prototype."
      ],
      "metadata": {
        "id": "O-fhIODZKOHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unified NST Gradio App with Metrics\n",
        "# AdaIN (TFHub) | VGG Optimizer | MobileNet Optimizer in one UI\n",
        "\n",
        "# Install all necessary packages\n",
        "!pip install -q tensorflow tensorflow_hub gradio opencv-python torch torchvision lpips scikit-image\n",
        "\n",
        "# Imports\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Pretrained models from TFHub\n",
        "import tensorflow_hub as hub\n",
        "# OpenCV for image processing\n",
        "import cv2\n",
        "import torch\n",
        "# Perceptual similarity metric\n",
        "import lpips\n",
        "# Pretrained models\n",
        "from tensorflow.keras.applications import vgg19, mobilenet_v2\n",
        "from tensorflow.keras.models import Model\n",
        "# Import image utilities\n",
        "from PIL import Image, ImageFilter\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Common helper functions\n",
        "\n",
        "# Convert any numpy image to float32 [0,1]\n",
        "def to_float01(np_img):\n",
        "    arr = np_img.astype(np.float32)\n",
        "    # If in [0,255] integers\n",
        "    if np_img.dtype == np.uint8:\n",
        "        # Normalize to [0,1]\n",
        "        arr = arr / 255.0\n",
        "    return np.clip(arr, 0, 1)\n",
        "\n",
        "\n",
        "# Create mask where near-black pixels are in the content's foreground\n",
        "def make_black_mask_from_np(content_np_resized, threshold=0.05):\n",
        "    # Check if ALL RGB channels at each pixel are below \"0.05\"\n",
        "    # Return \"True\" for pixels that are darker than the threshold in every channel\n",
        "    mask = np.all(content_np_resized < threshold, axis=-1).astype(np.float32)\n",
        "    return mask[..., None]\n",
        "\n",
        "\n",
        "# Preserve content colours (stylize luminance, keep chroma)\n",
        "def apply_colour_preservation(stylized_f01, content_resized_f01):\n",
        "    # OpenCV expects 8-bit inputs for colour conversion\n",
        "    # So, Convert both stylised and content images to uint8 [0,255]\n",
        "    stylized_u8 = (np.clip(stylized_f01,0,1)*255).astype(np.uint8)\n",
        "    content_u8  = (np.clip(content_resized_f01,0,1)*255).astype(np.uint8)\n",
        "    # Convert both style and content images from RGB to YUV colour space\n",
        "    stylized_yuv = cv2.cvtColor(stylized_u8, cv2.COLOR_RGB2YUV)\n",
        "    content_yuv  = cv2.cvtColor(content_u8, cv2.COLOR_RGB2YUV)\n",
        "    # Replace content’s luminance channel with stylized luminance channel to keep content colour\n",
        "    combined_yuv = content_yuv.copy()\n",
        "    combined_yuv[...,0] = stylized_yuv[...,0]\n",
        "    # Convert the final YUV image back to RGB\n",
        "    final_rgb = cv2.cvtColor(combined_yuv, cv2.COLOR_YUV2RGB)\n",
        "    return final_rgb.astype(np.float32)/255.0\n",
        "\n",
        "\n",
        "# Apply sharpness enhancement using UnsharpMask filter\n",
        "def apply_sharpness(stylized_f01, sharpness_value):\n",
        "    img_u8 = (np.clip(stylized_f01,0,1)*255).astype(np.uint8)\n",
        "    pil_img = Image.fromarray(img_u8)\n",
        "    pil_img = pil_img.filter(ImageFilter.UnsharpMask(\n",
        "        # Apply sharpen controls and filters\n",
        "        radius=1.5, percent=int(sharpness_value*200)))\n",
        "    return np.asarray(pil_img).astype(np.float32)/255.0\n",
        "\n",
        "\n",
        "# Collect style images with their weights and normalize weights\n",
        "def collect_styles_and_weights(styles, weights):\n",
        "    # Keep only non-empty styles\n",
        "    pairs = [(img,w) for img,w in zip(styles,weights) if img is not None]\n",
        "    if not pairs:\n",
        "        raise ValueError(\"Need ≥1 style image\")\n",
        "    # Separate images and weights\n",
        "    imgs, ws = zip(*pairs)\n",
        "    total = float(sum(ws))\n",
        "    if total <= 1e-8:\n",
        "        ws = [1.0] + [0.0]*(len(imgs)-1)\n",
        "    else:\n",
        "        ws = [float(w)/total for w in ws]\n",
        "    return list(imgs), ws\n",
        "\n",
        "# Computes SSIM and LPIPS metrics for evaluation\n",
        "# Load LPIPS model with AlexNet backbone\n",
        "lpips_model = lpips.LPIPS(net='alex').eval()\n",
        "\n",
        "# Compute SSIM and LPIPS metrics\n",
        "def compute_metrics(stylized_np, content_pil, style_imgs, blend_weights):\n",
        "    # Helper function to resize image to 256x256 RGB\n",
        "    def prep(im):\n",
        "        return cv2.resize(np.array(im.convert(\"RGB\")), (256,256))\n",
        "\n",
        "    # Prepare stylised and content images at 256x256\n",
        "    # This ensures all inputs are the same size before computing metrics\n",
        "    stylized_256 = prep(Image.fromarray((stylized_np*255).astype(np.uint8)))\n",
        "    content_256  = prep(content_pil)\n",
        "\n",
        "    # Compute weighted average blended style from all provided style images\n",
        "    blended_style_stack = np.zeros_like(stylized_256, dtype=np.float32)\n",
        "    for img,w in zip(style_imgs, blend_weights):\n",
        "        if img is not None:\n",
        "            blended_style_stack += prep(img).astype(np.float32) * w\n",
        "    # The final stack represents the reference style image against which LPIPS is measured\n",
        "    blended_style_stack = np.clip(blended_style_stack,0,255).astype(np.uint8)\n",
        "\n",
        "    # SSIM((Structural Similarity Index Measure)) metric to compute structural similarity with content\n",
        "    ssim_val = ssim(content_256, stylized_256, channel_axis=-1)\n",
        "\n",
        "    # Helper function to convert numpy image to Torch tensor in [-1,1]\n",
        "    def to_torch(np_img):\n",
        "        tens = torch.from_numpy(np_img.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0)*2-1\n",
        "        return tens\n",
        "\n",
        "    # Convert all three images (stylized, content, blended style) to Torch tensors\n",
        "    styl_t_img = to_torch(stylized_256)\n",
        "    cont_t_img = to_torch(content_256)\n",
        "    style_t= to_torch(blended_style_stack)\n",
        "\n",
        "    # LPIPS vs content\n",
        "    lpips_content = lpips_model(styl_t_img, cont_t_img).item()\n",
        "    # LPIPS vs style\n",
        "    lpips_style   = lpips_model(styl_t_img, style_t).item()\n",
        "    return ssim_val, lpips_content, lpips_style\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPOZszipMxW5",
        "outputId": "d84a73ad-55ae-4fc7-a025-bb8361696534"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AdaIN (TFHub Pretrained)\n",
        "# Load pretrained AdaIN model from TFHub\n",
        "adain_model = hub.load(\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\")\n",
        "\n",
        "# Convert PIL image to TensorFlow tensor [0,1]\n",
        "def load_image_from_upload(image, target_size=(256,256)):\n",
        "    # Resize to target size\n",
        "    image = image.convert(\"RGB\").resize(target_size)\n",
        "    # Normalize\n",
        "    img = np.array(image).astype(np.float32)/255.0\n",
        "    # Add batch dim\n",
        "    return tf.convert_to_tensor(img[None,...], dtype=tf.float32)\n",
        "\n",
        "\n",
        "# Blend multiple styles with AdaIN model\n",
        "def blend_styles_with_adain(content_img, style_imgs, weights):\n",
        "    # Normalize weights\n",
        "    weights = np.array(weights); weights = weights/weights.sum()\n",
        "    # Content tensor\n",
        "    content_tensor = load_image_from_upload(content_img)\n",
        "    # Style tensors\n",
        "    style_tensors = [load_image_from_upload(img) for img in style_imgs]\n",
        "\n",
        "    # Weighted blend of styles\n",
        "    blended = tf.zeros_like(style_tensors[0])\n",
        "    for i,st in enumerate(style_tensors):\n",
        "        blended += st*weights[i]\n",
        "\n",
        "    # Run AdaIN model\n",
        "    stylized = adain_model(content_tensor, blended)[0]\n",
        "    # Remove batch dimension\n",
        "    out = tf.squeeze(stylized).numpy()\n",
        "    out = np.clip(out*255,0,255).astype(\"uint8\")\n",
        "    return Image.fromarray(out)\n",
        "\n",
        "\n",
        "# Gradio callback for AdaIN stylization\n",
        "def stylize_ui_adain(content,s1,s2,s3,s4,s5,\n",
        "                     w1,w2,w3,w4,w5,\n",
        "                     alpha,beta,preserve_colour,foreground_only,sharpness):\n",
        "    # Collect styles and normalize weights\n",
        "    style_imgs,blend_weights = collect_styles_and_weights([s1,s2,s3,s4,s5],[w1,w2,w3,w4,w5])\n",
        "\n",
        "    # Run AdaIN with blended styles\n",
        "    out = np.array(blend_styles_with_adain(content,style_imgs,blend_weights)).astype(np.float32)/255.0\n",
        "\n",
        "    # Resize both stylized and content to 512x512\n",
        "    stylized_resized = cv2.resize((out*255).astype(np.uint8), (512,512),\n",
        "                                  interpolation=cv2.INTER_CUBIC).astype(np.float32)/255.0\n",
        "    content_resized  = np.array(content.resize((512,512))).astype(np.float32)/255.0\n",
        "\n",
        "    # Apply optional extras\n",
        "    # Keep content colours\n",
        "    if preserve_colour:\n",
        "        stylized_resized = apply_colour_preservation(stylized_resized, content_resized)\n",
        "    # Add sharpness controls\n",
        "    if sharpness>0:\n",
        "        stylized_resized = apply_sharpness(stylized_resized, sharpness)\n",
        "    # Apply style only on foreground\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        stylized_resized = mask*stylized_resized + (1.0-mask)*content_resized\n",
        "\n",
        "    return np.clip(stylized_resized,0,1)\n"
      ],
      "metadata": {
        "id": "a0JADhweM9Xg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build VGG Optimizer\n",
        "\n",
        "# Extract selected VGG layers\n",
        "def vgg_layers(layer_names):\n",
        "    # Load VGG19 model\n",
        "    vgg = vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    # Collect requested layers\n",
        "    outputs = [vgg.get_layer(n).output for n in layer_names]\n",
        "    return Model([vgg.input], outputs)\n",
        "\n",
        "\n",
        "# Compute Gram matrix (for style loss)\n",
        "def gram_matrix(x):\n",
        "    # Inner product\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
        "    # Normalise factor\n",
        "    num = tf.cast(tf.shape(x)[1]*tf.shape(x)[2], tf.float32)\n",
        "    return result / num\n",
        "\n",
        "\n",
        "# Style and Content feature extractor using VGG\n",
        "class VGGStyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super().__init__()\n",
        "        # Load model with style and content layers\n",
        "        self.vgg = vgg_layers(style_layers+content_layers)\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "    def call(self, inputs):\n",
        "        inputs = inputs*255.0\n",
        "        # VGG preprocessing\n",
        "        pre = vgg19.preprocess_input(inputs)\n",
        "        outs = self.vgg(pre)\n",
        "        s_outs, c_outs = outs[:self.num_style], outs[self.num_style:]\n",
        "        # Convert style feats to Gram matrices\n",
        "        s_outs = [gram_matrix(s) for s in s_outs]\n",
        "        return {'style':{n:v for n,v in zip(self.style_layers,s_outs)},\n",
        "                'content':{n:v for n,v in zip(self.content_layers,c_outs)}}\n",
        "\n",
        "\n",
        "# Preprocess PIL image for VGG [0,1] → batch tensor\n",
        "def preprocess_img_vgg(pil_img):\n",
        "    img = np.array(pil_img).astype(np.float32)/255.0\n",
        "    img = tf.image.resize(img,(512,512))\n",
        "    return img[tf.newaxis,:]\n",
        "\n",
        "\n",
        "# Blend multiple style targets\n",
        "def blend_style_targets(style_images, extractor, blend_weights):\n",
        "    blended = None\n",
        "    for i,img in enumerate(style_images):\n",
        "        target = extractor(preprocess_img_vgg(img))['style']\n",
        "        # Initialize with weighted first style\n",
        "        if blended is None:\n",
        "            blended = {k:v*blend_weights[i] for k,v in target.items()}\n",
        "        # Add weighted styles to existing dict\n",
        "        else:\n",
        "            [blended.update({k:blended[k]+target[k]*blend_weights[i]}) for k in blended]\n",
        "    return blended\n",
        "\n",
        "\n",
        "# Optimization loop for VGG-based NST\n",
        "def run_style_transfer_vgg(content, style_imgs, blend_weights, alpha, beta):\n",
        "    content_in = preprocess_img_vgg(content)\n",
        "    extractor = VGGStyleContentModel(\n",
        "        # style layers\n",
        "        ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1'],\n",
        "        # content layer\n",
        "        ['block5_conv2'])\n",
        "    targets = {\n",
        "        'style': blend_style_targets(style_imgs, extractor, blend_weights),\n",
        "        'content': extractor(content_in)['content']\n",
        "    }\n",
        "\n",
        "    # Start with content image as variable\n",
        "    img = tf.Variable(content_in)\n",
        "    # Adam optimizer\n",
        "    opt = tf.optimizers.Adam(0.02)\n",
        "\n",
        "    # 50 iterations optimization\n",
        "    for _ in range(50):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = extractor(img)\n",
        "            s_loss = tf.add_n([tf.reduce_mean((out['style'][k]-targets['style'][k])**2) for k in targets['style']])\n",
        "            c_loss = tf.add_n([tf.reduce_mean((out['content'][k]-targets['content'][k])**2) for k in targets['content']])\n",
        "            loss = alpha*c_loss + beta*s_loss\n",
        "        # Compute gradients\n",
        "        grad = tape.gradient(loss, img)\n",
        "        # Apply update\n",
        "        opt.apply_gradients([(grad,img)])\n",
        "        # Keep valid range\n",
        "        img.assign(tf.clip_by_value(img,0,1))\n",
        "    return tf.squeeze(img).numpy()\n",
        "\n",
        "\n",
        "# Gradio callback for VGG optimizer stylization\n",
        "def stylize_ui_vgg(content,s1,s2,s3,s4,s5,\n",
        "                   w1,w2,w3,w4,w5,\n",
        "                   alpha,beta,preserve_colour,foreground_only,sharpness):\n",
        "    # Collect style images and weights\n",
        "    style_imgs,blend_weights = collect_styles_and_weights([s1,s2,s3,s4,s5],[w1,w2,w3,w4,w5])\n",
        "    # Run optimization\n",
        "    out = run_style_transfer_vgg(content, style_imgs, blend_weights, alpha, beta)\n",
        "    # Resize content for extras\n",
        "    content_resized = np.array(content.resize((512,512))).astype(np.float32)/255.0\n",
        "\n",
        "    # Apply optional extras\n",
        "    if preserve_colour:\n",
        "        out = apply_colour_preservation(out, content_resized)\n",
        "    if sharpness>0:\n",
        "        out = apply_sharpness(out, sharpness)\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        out = mask*out + (1.0-mask)*content_resized\n",
        "    return np.clip(out,0,1)\n"
      ],
      "metadata": {
        "id": "MI6aHE1QNRzV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet Optimizer (Multi-stage)\n",
        "\n",
        "# Define the style layers we want to extract from MobileNetV2\n",
        "STYLE_LAYERS = [\n",
        "    'block_1_expand_relu','block_3_expand_relu','block_6_expand_relu',\n",
        "    'block_10_expand_relu','block_13_expand_relu'\n",
        "]\n",
        "\n",
        "# Assign different weights to each style layer (lower layers capture fine textures, deeper layers capture structure)\n",
        "STYLE_LAYER_WEIGHTS = {\n",
        "    # Early layer has the highest weight\n",
        "    'block_1_expand_relu':1.0,\n",
        "    'block_3_expand_relu':0.8,\n",
        "    'block_6_expand_relu':0.6,\n",
        "    'block_10_expand_relu':0.5,\n",
        "    # Deepest style layer has the lowest weight\n",
        "    'block_13_expand_relu':0.4\n",
        "}\n",
        "\n",
        "# Define the content layers (deep layers preserve high-level structure)\n",
        "CONTENT_LAYERS = ['block_13_expand_relu','block_16_project']\n",
        "\n",
        "\n",
        "# MobileNet feature extractor class\n",
        "class MNetStyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, img_size):\n",
        "        super().__init__()\n",
        "        # Store style and content layer names\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "\n",
        "        # Load pretrained MobileNetV2\n",
        "        base = mobilenet_v2.MobileNetV2(include_top=False,\n",
        "                                        weights='imagenet',\n",
        "                                        input_shape=(img_size,img_size,3))\n",
        "        base.trainable = False\n",
        "\n",
        "        # Collect outputs from specified style and content layers\n",
        "        outs = [base.get_layer(n).output for n in style_layers+content_layers]\n",
        "\n",
        "        # Build encoder model that outputs these intermediate activations\n",
        "        self.encoder = Model([base.input], outs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Pass input image through MobileNet encoder\n",
        "        outs = self.encoder(x)\n",
        "\n",
        "        # Compute Gram matrices for style outputs (first part of outs)\n",
        "        style_outs = [gram_matrix(o) for o in outs[:self.num_style]]\n",
        "\n",
        "        # Return dictionary with style and content features\n",
        "        return {\n",
        "            'style': {n:v for n,v in zip(self.style_layers, style_outs)},\n",
        "            'content': {n:v for n,v in zip(self.content_layers, outs[self.num_style:])}\n",
        "        }\n",
        "\n",
        "\n",
        "# Preprocess input PIL image for MobileNetV2\n",
        "def preprocess_for_model(pil_img, img_size):\n",
        "   # Convert PIL → numpy [0,1]\n",
        "    x = np.array(pil_img).astype(np.float32)/255.0\n",
        "    # Resize to target size\n",
        "    x = tf.image.resize(x, (img_size,img_size), antialias=True)\n",
        "    # Apply MobileNet-specific preprocessing\n",
        "    x = mobilenet_v2.preprocess_input(x*255.0)\n",
        "    return x[None,...]\n",
        "\n",
        "\n",
        "# Convert MobileNet output ([-1,1]) back to [0,1] float image\n",
        "def deprocess_from_model(x_minus1_to1):\n",
        "    # Remove batch dimension if present\n",
        "    x = tf.squeeze(x_minus1_to1,0)\n",
        "    x = tf.clip_by_value((x+1.0)/2.0, 0.0, 1.0)\n",
        "    return x.numpy().astype(np.float32)\n",
        "\n",
        "\n",
        "# Blend multiple style targets for MobileNet\n",
        "def blended_style_targets(extractor, style_pils, blend_weights, img_size):\n",
        "    accum = None\n",
        "    for img, w in zip(style_pils, blend_weights):\n",
        "        # Extract style features for current image\n",
        "        st = extractor(preprocess_for_model(img, img_size))['style']\n",
        "        # First style image to initialise accumulator\n",
        "        if accum is None:\n",
        "            accum = {k: v*w for k,v in st.items()}\n",
        "        # Add weighted style features to accumulator\n",
        "        else:\n",
        "            [accum.update({k: accum[k] + st[k]*w}) for k in accum]\n",
        "\n",
        "    # Return weighted blended style targets\n",
        "    return accum\n",
        "\n",
        "\n",
        "# Laplacian operator for edge loss\n",
        "\n",
        "def laplacian(img_minus1_to1):\n",
        "    # Convert input image from [-1, 1] range to [0, 1]\n",
        "    x = (img_minus1_to1 + 1.0) / 2.0\n",
        "\n",
        "    # Define Laplacian kernel (edge detector)\n",
        "    k = tf.constant([[0., -1., 0.],\n",
        "                     [-1., 4., -1.],\n",
        "                     [0., -1., 0.]], tf.float32)\n",
        "\n",
        "    # Reshape kernel to 4D shape [H,W,in_channels,out_channels]\n",
        "    k = tf.reshape(k, [3, 3, 1, 1])\n",
        "\n",
        "    # Repeat kernel for all 3 RGB channels\n",
        "    k = tf.repeat(k, 3, axis=2)\n",
        "\n",
        "    # Apply convolution (SAME padding keeps size)\n",
        "    return tf.nn.conv2d(x, k, strides=1, padding='SAME')\n",
        "\n",
        "\n",
        "# Multi-stage optimization loop (224 → 512 → 768 )\n",
        "\n",
        "def run_stage(content_pil, style_pils, blend_weights, img_size,\n",
        "              steps, lr_start, lr_end, alpha_content, beta_style,\n",
        "              tv_weight=1e-6, edge_weight=5e-3, init_from=None):\n",
        "\n",
        "    # Preprocess the content image to MobileNet format at target resolution\n",
        "    content = preprocess_for_model(content_pil, img_size)\n",
        "\n",
        "    # Initialize MobileNet-based extractor with style/content layers\n",
        "    extractor = MNetStyleContentModel(STYLE_LAYERS, CONTENT_LAYERS, img_size)\n",
        "\n",
        "    # Get blended style features from all style images\n",
        "    style_tgt = blended_style_targets(extractor, style_pils, blend_weights, img_size)\n",
        "\n",
        "    # Get content target features\n",
        "    content_tgt = extractor(content)['content']\n",
        "\n",
        "    # If no initialization image is passed\n",
        "    if init_from is None:\n",
        "        # Use weighted mix of content and first style image as seed\n",
        "        style_seed = preprocess_for_model(style_pils[0], img_size)\n",
        "        seed = tf.clip_by_value(((content + 1) / 2.0) * 0.6 + ((style_seed + 1) / 2.0) * 0.4, 0, 1)\n",
        "        image = tf.Variable(mobilenet_v2.preprocess_input(seed * 255.0))\n",
        "    else:\n",
        "        # Upscale previous stage output to current size and use it as init\n",
        "        up = tf.image.resize(init_from, (img_size, img_size), method='bicubic', antialias=True)\n",
        "        image = tf.Variable(up)\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(lr_start, steps, alpha=lr_end / lr_start)\n",
        "    opt = tf.optimizers.Adam(lr_schedule)\n",
        "\n",
        "    # Normalize style loss by total style weights\n",
        "    denom = sum(STYLE_LAYER_WEIGHTS.values())\n",
        "\n",
        "    # Define one training step\n",
        "    @tf.function\n",
        "    def train_step(img):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Extract features from current image\n",
        "            outs = extractor(img)\n",
        "            s, c = outs['style'], outs['content']\n",
        "\n",
        "            # Compute style loss (weighted MSE across layers)\n",
        "            s_loss = tf.add_n([\n",
        "                STYLE_LAYER_WEIGHTS[k] * tf.reduce_mean((s[k] - style_tgt[k]) ** 2)\n",
        "                for k in s\n",
        "            ]) * (beta_style / denom)\n",
        "\n",
        "            # Compute content loss (MSE across layers)\n",
        "            c_loss = tf.add_n([\n",
        "                tf.reduce_mean((c[k] - content_tgt[k]) ** 2)\n",
        "                for k in c\n",
        "            ]) * (alpha_content / len(CONTENT_LAYERS))\n",
        "\n",
        "            # Add total variation loss (encourages smoothness)\n",
        "            tv = tf.reduce_mean(tf.image.total_variation(img)) * tv_weight\n",
        "\n",
        "            # Add edge loss (match Laplacian edges of content and stylized)\n",
        "            edge = tf.reduce_mean((laplacian(img) - laplacian(content)) ** 2) * edge_weight\n",
        "\n",
        "            # Final total loss\n",
        "            loss = s_loss + c_loss + tv + edge\n",
        "\n",
        "        # Compute gradients of loss wrt. image\n",
        "        grad = tape.gradient(loss, img)\n",
        "\n",
        "        # Clip gradients to avoid exploding updates\n",
        "        grad = tf.clip_by_norm(grad, 10.0)\n",
        "\n",
        "        # Apply optimizer step\n",
        "        opt.apply_gradients([(grad, img)])\n",
        "\n",
        "        # Clamp image values back into valid range [-1, 1]\n",
        "        img.assign(tf.clip_by_value(img, -1, 1))\n",
        "        return loss\n",
        "\n",
        "    # Run training loop for the specified number of steps\n",
        "    for _ in range(steps):\n",
        "        _ = train_step(image)\n",
        "\n",
        "    # Return optimized image tensor\n",
        "    return image\n",
        "\n",
        "# Gradio callback for MobileNet style transfer run\n",
        "\n",
        "def stylize_ui_mnet(content, s1, s2, s3, s4, s5,\n",
        "                    w1, w2, w3, w4, w5,\n",
        "                    alpha, beta, preserve_colour, foreground_only, sharpness):\n",
        "\n",
        "    # Collect style images and normalize weights\n",
        "    style_imgs, blend_weights = collect_styles_and_weights(\n",
        "        [s1, s2, s3, s4, s5],\n",
        "        [w1, w2, w3, w4, w5]\n",
        "    )\n",
        "\n",
        "    # Stage 1: Optimize at 224x224 resolution\n",
        "    out_224 = run_stage(content, style_imgs, blend_weights, 224, 400, 0.05, 0.01, alpha, beta)\n",
        "    # Stage 2: Optimize at 512x512 resolution, initialized from stage 1\n",
        "    out_512 = run_stage(content, style_imgs, blend_weights, 512, 350, 0.03, 0.006, alpha * 2, beta, init_from=out_224)\n",
        "    # Stage 3: Optimize at 768x768 resolution, initialized from stage 2\n",
        "    out_768 = run_stage(content, style_imgs, blend_weights, 768, 300, 0.02, 0.004, alpha * 3, beta, init_from=out_512)\n",
        "\n",
        "    # Convert final output tensor back to [0,1] float image\n",
        "    stylized = deprocess_from_model(out_768)\n",
        "\n",
        "    # Resize final result to 512x512 for UI consistency\n",
        "    stylized = tf.image.resize(stylized, (512, 512), antialias=True, method='bicubic').numpy()\n",
        "\n",
        "    # Convert content image to float [0,1] numpy\n",
        "    content_np = to_float01(np.array(content))\n",
        "\n",
        "    # Resize content to 512x512 for post-processing steps\n",
        "    content_resized = np.array(\n",
        "        Image.fromarray((content_np * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)\n",
        "    ).astype(np.float32) / 255.0\n",
        "\n",
        "    # If user enabled colour preservation\n",
        "    if preserve_colour:\n",
        "        # Retain colours from the original content image and combine with the textures from the stylised image\n",
        "        stylized = apply_colour_preservation(stylized, content_resized)\n",
        "\n",
        "    # If user enabled sharpness, apply unsharp mask filter\n",
        "    if sharpness > 0:\n",
        "        stylized = apply_sharpness(stylized, sharpness)\n",
        "\n",
        "    # If user enabled foreground-only styling, mask out background\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        stylized = mask * stylized + (1.0 - mask) * content_resized\n",
        "\n",
        "    # Return final image clipped into [0,1]\n",
        "    return np.clip(stylized, 0, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "l94DNayGM-sT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement final 3-in-1 Gradio NST App\n",
        "# Dispatch routes request to chosen NST model\n",
        "def unified_stylize(model_choice, content,\n",
        "                    s1, s2, s3, s4, s5,\n",
        "                    w1, w2, w3, w4, w5,\n",
        "                    alpha, beta, preserve_colour, foreground_only, sharpness):\n",
        "\n",
        "    # Run AdaIN model if selected\n",
        "    if model_choice == \"AdaIN\":\n",
        "        out = stylize_ui_adain(content, s1, s2, s3, s4, s5,\n",
        "                               w1, w2, w3, w4, w5,\n",
        "                               alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Run VGG optimizer if selected\n",
        "    elif model_choice == \"VGG Optimizer\":\n",
        "        out = stylize_ui_vgg(content, s1, s2, s3, s4, s5,\n",
        "                             w1, w2, w3, w4, w5,\n",
        "                             alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Run MobileNet optimizer if selected\n",
        "    elif model_choice == \"MobileNet Optimizer\":\n",
        "        out = stylize_ui_mnet(content, s1, s2, s3, s4, s5,\n",
        "                              w1, w2, w3, w4, w5,\n",
        "                              alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Compute SSIM and LPIPS metrics for evaluation\n",
        "    style_imgs, blend_weights = collect_styles_and_weights([s1, s2, s3, s4, s5],\n",
        "                                                      [w1, w2, w3, w4, w5])\n",
        "    ssim_val, lpips_c, lpips_s = compute_metrics(out, content, style_imgs, blend_weights)\n",
        "\n",
        "    # Return stylised image and metrics\n",
        "    return out, ssim_val, lpips_c, lpips_s\n",
        "\n",
        "# Gradio User Interface\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # Title and description\n",
        "    # Higher SSIM , better performance\n",
        "    # Lower LPIPS , better performance\n",
        "    gr.Markdown(\"##  3-in-1 NST App : AdaIN | VGG Optimizer | MobileNet Optimizer\")\n",
        "    gr.Markdown(\n",
        "        \"After you click \\\"Stylise\\\" — your chosen model makes the stylised image, \"\n",
        "        \"then metrics are computed:\\n\"\n",
        "    )\n",
        "\n",
        "    # Create dropdown menu for model selection\n",
        "    model_choice = gr.Dropdown(\n",
        "        [\"AdaIN\", \"VGG Optimizer\", \"MobileNet Optimizer\"],\n",
        "        value=\"AdaIN\",\n",
        "        label=\"Select Model\"\n",
        "    )\n",
        "\n",
        "    # Upload option for content image\n",
        "    content_input = gr.Image(\n",
        "        label=\"Content Image\",\n",
        "        type=\"pil\",\n",
        "        image_mode=\"RGB\",\n",
        "        height=256,\n",
        "        width=256\n",
        "    )\n",
        "\n",
        "    # Row of 5 style image upload option facilities\n",
        "    with gr.Row():\n",
        "        style_input1 = gr.Image(label=\"Style 1\", type=\"pil\")\n",
        "        style_input2 = gr.Image(label=\"Style 2\", type=\"pil\")\n",
        "        style_input3 = gr.Image(label=\"Style 3\", type=\"pil\")\n",
        "        style_input4 = gr.Image(label=\"Style 4\", type=\"pil\")\n",
        "        style_input5 = gr.Image(label=\"Style 5\", type=\"pil\")\n",
        "\n",
        "    # Initialise Sliders to adjust style weights of 5 style images\n",
        "    with gr.Row():\n",
        "        blend1 = gr.Slider(0, 1, 1, step=0.01, label=\"Style 1 Strength\")\n",
        "        blend2 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 2 Strength\")\n",
        "        blend3 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 3 Strength\")\n",
        "        blend4 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 4 Strength\")\n",
        "        blend5 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 5 Strength\")\n",
        "\n",
        "    # Controls for content/style weights , colour preservation , foreground styling and sharpness controls\n",
        "    with gr.Row():\n",
        "        alpha_slider = gr.Slider(1, 5000, 1000, step=10, label=\"α (Content Weight)\")\n",
        "        beta_slider = gr.Slider(1e-3, 1.0, 1e-2, step=1e-3, label=\"β (Style Weight)\")\n",
        "        preserve_colour = gr.Checkbox(label=\"Apply Colour Preservation\", value=False)\n",
        "        foreground_only = gr.Checkbox(label=\"Apply Foreground-Aware Styling\", value=False)\n",
        "        sharpness = gr.Slider(0, 1, 0.5, step=0.05, label=\"Style Sharpness\")\n",
        "\n",
        "    # Run button\n",
        "    run_btn = gr.Button(\"Stylise\")\n",
        "\n",
        "    # Output row to display final image and metrics\n",
        "    with gr.Row():\n",
        "        output_img = gr.Image(label=\"Stylised Output\", image_mode=\"RGB\", height=512, width=512)\n",
        "        with gr.Column():\n",
        "            ssim_metric = gr.Number(label=\"SSIM vs Content (↑ better)\")\n",
        "            lpips_content_metric = gr.Number(label=\"LPIPS vs Content (↓ better)\")\n",
        "            lpips_style_metric = gr.Number(label=\"LPIPS vs Blended Style (↓ better)\")\n",
        "\n",
        "    # Connect button to unified stylisation function\n",
        "    run_btn.click(\n",
        "        fn=unified_stylize,\n",
        "        inputs=[model_choice, content_input,\n",
        "                style_input1, style_input2, style_input3, style_input4, style_input5,\n",
        "                blend1, blend2, blend3, blend4, blend5,\n",
        "                alpha_slider, beta_slider, preserve_colour, foreground_only, sharpness],\n",
        "        outputs=[output_img, ssim_metric, lpips_content_metric, lpips_style_metric]\n",
        "    )\n",
        "\n",
        "# Launch app\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "jXbgYya2Ntev",
        "outputId": "25d000c4-8b3f-409f-81f4-28d3c26a4db6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bcf51045289b1184fd.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bcf51045289b1184fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "YOeAZB8T8iWT",
        "outputId": "48a8c3b8-de34-48e5-86a4-c2bcf39c27de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hSetting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 173MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://6da973ff3997231637.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6da973ff3997231637.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#FULL CODE COMPILED TO ONE CELL FOR EASY RUN\n",
        "# Unified NST Gradio App with Metrics\n",
        "# AdaIN (TFHub) | VGG Optimizer | MobileNet Optimizer in one UI\n",
        "\n",
        "# Install all necessary packages\n",
        "!pip install -q tensorflow tensorflow_hub gradio opencv-python torch torchvision lpips scikit-image\n",
        "\n",
        "# Imports\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# Pretrained models from TFHub\n",
        "import tensorflow_hub as hub\n",
        "# OpenCV for image processing\n",
        "import cv2\n",
        "import torch\n",
        "# Perceptual similarity metric\n",
        "import lpips\n",
        "# Pretrained models\n",
        "from tensorflow.keras.applications import vgg19, mobilenet_v2\n",
        "from tensorflow.keras.models import Model\n",
        "# Import image utilities\n",
        "from PIL import Image, ImageFilter\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "\n",
        "# Common helper functions\n",
        "\n",
        "# Convert any numpy image to float32 [0,1]\n",
        "def to_float01(np_img):\n",
        "    arr = np_img.astype(np.float32)\n",
        "    # If in [0,255] integers\n",
        "    if np_img.dtype == np.uint8:\n",
        "        # Normalize to [0,1]\n",
        "        arr = arr / 255.0\n",
        "    return np.clip(arr, 0, 1)\n",
        "\n",
        "\n",
        "# Create mask where near-black pixels are in the content's foreground\n",
        "def make_black_mask_from_np(content_np_resized, threshold=0.05):\n",
        "    # Check if ALL RGB channels at each pixel are below \"0.05\"\n",
        "    # Return \"True\" for pixels that are darker than the threshold in every channel\n",
        "    mask = np.all(content_np_resized < threshold, axis=-1).astype(np.float32)\n",
        "    return mask[..., None]\n",
        "\n",
        "\n",
        "# Preserve content colours (stylize luminance, keep chroma)\n",
        "def apply_colour_preservation(stylized_f01, content_resized_f01):\n",
        "    # OpenCV expects 8-bit inputs for colour conversion\n",
        "    # So, Convert both stylised and content images to uint8 [0,255]\n",
        "    stylized_u8 = (np.clip(stylized_f01,0,1)*255).astype(np.uint8)\n",
        "    content_u8  = (np.clip(content_resized_f01,0,1)*255).astype(np.uint8)\n",
        "    # Convert both style and content images from RGB to YUV colour space\n",
        "    stylized_yuv = cv2.cvtColor(stylized_u8, cv2.COLOR_RGB2YUV)\n",
        "    content_yuv  = cv2.cvtColor(content_u8, cv2.COLOR_RGB2YUV)\n",
        "    # Replace content’s luminance channel with stylized luminance channel to keep content colour\n",
        "    combined_yuv = content_yuv.copy()\n",
        "    combined_yuv[...,0] = stylized_yuv[...,0]\n",
        "    # Convert the final YUV image back to RGB\n",
        "    final_rgb = cv2.cvtColor(combined_yuv, cv2.COLOR_YUV2RGB)\n",
        "    return final_rgb.astype(np.float32)/255.0\n",
        "\n",
        "\n",
        "# Apply sharpness enhancement using UnsharpMask filter\n",
        "def apply_sharpness(stylized_f01, sharpness_value):\n",
        "    img_u8 = (np.clip(stylized_f01,0,1)*255).astype(np.uint8)\n",
        "    pil_img = Image.fromarray(img_u8)\n",
        "    pil_img = pil_img.filter(ImageFilter.UnsharpMask(\n",
        "        # Apply sharpen controls and filters\n",
        "        radius=1.5, percent=int(sharpness_value*200)))\n",
        "    return np.asarray(pil_img).astype(np.float32)/255.0\n",
        "\n",
        "\n",
        "# Collect style images with their weights and normalize weights\n",
        "def collect_styles_and_weights(styles, weights):\n",
        "    # Keep only non-empty styles\n",
        "    pairs = [(img,w) for img,w in zip(styles,weights) if img is not None]\n",
        "    if not pairs:\n",
        "        raise ValueError(\"Need ≥1 style image\")\n",
        "    # Separate images and weights\n",
        "    imgs, ws = zip(*pairs)\n",
        "    total = float(sum(ws))\n",
        "    if total <= 1e-8:\n",
        "        ws = [1.0] + [0.0]*(len(imgs)-1)\n",
        "    else:\n",
        "        ws = [float(w)/total for w in ws]\n",
        "    return list(imgs), ws\n",
        "\n",
        "# Computes SSIM and LPIPS metrics for evaluation\n",
        "# Load LPIPS model with AlexNet backbone\n",
        "lpips_model = lpips.LPIPS(net='alex').eval()\n",
        "\n",
        "# Compute SSIM and LPIPS metrics\n",
        "def compute_metrics(stylized_np, content_pil, style_imgs, blend_weights):\n",
        "    # Helper function to resize image to 256x256 RGB\n",
        "    def prep(im):\n",
        "        return cv2.resize(np.array(im.convert(\"RGB\")), (256,256))\n",
        "\n",
        "    # Prepare stylised and content images at 256x256\n",
        "    # This ensures all inputs are the same size before computing metrics\n",
        "    stylized_256 = prep(Image.fromarray((stylized_np*255).astype(np.uint8)))\n",
        "    content_256  = prep(content_pil)\n",
        "\n",
        "    # Compute weighted average blended style from all provided style images\n",
        "    blended_style_stack = np.zeros_like(stylized_256, dtype=np.float32)\n",
        "    for img,w in zip(style_imgs, blend_weights):\n",
        "        if img is not None:\n",
        "            blended_style_stack += prep(img).astype(np.float32) * w\n",
        "    # The final stack represents the reference style image against which LPIPS is measured\n",
        "    blended_style_stack = np.clip(blended_style_stack,0,255).astype(np.uint8)\n",
        "\n",
        "    # SSIM((Structural Similarity Index Measure)) metric to compute structural similarity with content\n",
        "    ssim_val = ssim(content_256, stylized_256, channel_axis=-1)\n",
        "\n",
        "    # Helper function to convert numpy image to Torch tensor in [-1,1]\n",
        "    def to_torch(np_img):\n",
        "        tens = torch.from_numpy(np_img.astype(np.float32)/255.0).permute(2,0,1).unsqueeze(0)*2-1\n",
        "        return tens\n",
        "\n",
        "    # Convert all three images (stylized, content, blended style) to Torch tensors\n",
        "    styl_t_img = to_torch(stylized_256)\n",
        "    cont_t_img = to_torch(content_256)\n",
        "    style_t= to_torch(blended_style_stack)\n",
        "\n",
        "    # LPIPS vs content\n",
        "    lpips_content = lpips_model(styl_t_img, cont_t_img).item()\n",
        "    # LPIPS vs style\n",
        "    lpips_style   = lpips_model(styl_t_img, style_t).item()\n",
        "    return ssim_val, lpips_content, lpips_style\n",
        "\n",
        "\n",
        "# AdaIN (TFHub Pretrained)\n",
        "# Load pretrained AdaIN model from TFHub\n",
        "adain_model = hub.load(\"https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2\")\n",
        "\n",
        "# Convert PIL image to TensorFlow tensor [0,1]\n",
        "def load_image_from_upload(image, target_size=(256,256)):\n",
        "    # Resize to target size\n",
        "    image = image.convert(\"RGB\").resize(target_size)\n",
        "    # Normalize\n",
        "    img = np.array(image).astype(np.float32)/255.0\n",
        "    # Add batch dim\n",
        "    return tf.convert_to_tensor(img[None,...], dtype=tf.float32)\n",
        "\n",
        "\n",
        "# Blend multiple styles with AdaIN model\n",
        "def blend_styles_with_adain(content_img, style_imgs, weights):\n",
        "    # Normalize weights\n",
        "    weights = np.array(weights); weights = weights/weights.sum()\n",
        "    # Content tensor\n",
        "    content_tensor = load_image_from_upload(content_img)\n",
        "    # Style tensors\n",
        "    style_tensors = [load_image_from_upload(img) for img in style_imgs]\n",
        "\n",
        "    # Weighted blend of styles\n",
        "    blended = tf.zeros_like(style_tensors[0])\n",
        "    for i,st in enumerate(style_tensors):\n",
        "        blended += st*weights[i]\n",
        "\n",
        "    # Run AdaIN model\n",
        "    stylized = adain_model(content_tensor, blended)[0]\n",
        "    # Remove batch dimension\n",
        "    out = tf.squeeze(stylized).numpy()\n",
        "    out = np.clip(out*255,0,255).astype(\"uint8\")\n",
        "    return Image.fromarray(out)\n",
        "\n",
        "\n",
        "# Gradio callback for AdaIN stylization\n",
        "def stylize_ui_adain(content,s1,s2,s3,s4,s5,\n",
        "                     w1,w2,w3,w4,w5,\n",
        "                     alpha,beta,preserve_colour,foreground_only,sharpness):\n",
        "    # Collect styles and normalize weights\n",
        "    style_imgs,blend_weights = collect_styles_and_weights([s1,s2,s3,s4,s5],[w1,w2,w3,w4,w5])\n",
        "\n",
        "    # Run AdaIN with blended styles\n",
        "    out = np.array(blend_styles_with_adain(content,style_imgs,blend_weights)).astype(np.float32)/255.0\n",
        "\n",
        "    # Resize both stylized and content to 512x512\n",
        "    stylized_resized = cv2.resize((out*255).astype(np.uint8), (512,512),\n",
        "                                  interpolation=cv2.INTER_CUBIC).astype(np.float32)/255.0\n",
        "    content_resized  = np.array(content.resize((512,512))).astype(np.float32)/255.0\n",
        "\n",
        "    # Apply optional extras\n",
        "    # Keep content colours\n",
        "    if preserve_colour:\n",
        "        stylized_resized = apply_colour_preservation(stylized_resized, content_resized)\n",
        "    # Add sharpness controls\n",
        "    if sharpness>0:\n",
        "        stylized_resized = apply_sharpness(stylized_resized, sharpness)\n",
        "    # Apply style only on foreground\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        stylized_resized = mask*stylized_resized + (1.0-mask)*content_resized\n",
        "\n",
        "    return np.clip(stylized_resized,0,1)\n",
        "\n",
        "\n",
        "# Build VGG Optimizer\n",
        "\n",
        "# Extract selected VGG layers\n",
        "def vgg_layers(layer_names):\n",
        "    # Load VGG19 model\n",
        "    vgg = vgg19.VGG19(include_top=False,weights='imagenet')\n",
        "    vgg.trainable = False\n",
        "    # Collect requested layers\n",
        "    outputs = [vgg.get_layer(n).output for n in layer_names]\n",
        "    return Model([vgg.input], outputs)\n",
        "\n",
        "\n",
        "# Compute Gram matrix (for style loss)\n",
        "def gram_matrix(x):\n",
        "    # Inner product\n",
        "    result = tf.linalg.einsum('bijc,bijd->bcd', x, x)\n",
        "    # Normalise factor\n",
        "    num = tf.cast(tf.shape(x)[1]*tf.shape(x)[2], tf.float32)\n",
        "    return result / num\n",
        "\n",
        "\n",
        "# Style and Content feature extractor using VGG\n",
        "class VGGStyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers):\n",
        "        super().__init__()\n",
        "        # Load model with style and content layers\n",
        "        self.vgg = vgg_layers(style_layers+content_layers)\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "    def call(self, inputs):\n",
        "        inputs = inputs*255.0\n",
        "        # VGG preprocessing\n",
        "        pre = vgg19.preprocess_input(inputs)\n",
        "        outs = self.vgg(pre)\n",
        "        s_outs, c_outs = outs[:self.num_style], outs[self.num_style:]\n",
        "        # Convert style feats to Gram matrices\n",
        "        s_outs = [gram_matrix(s) for s in s_outs]\n",
        "        return {'style':{n:v for n,v in zip(self.style_layers,s_outs)},\n",
        "                'content':{n:v for n,v in zip(self.content_layers,c_outs)}}\n",
        "\n",
        "\n",
        "# Preprocess PIL image for VGG [0,1] → batch tensor\n",
        "def preprocess_img_vgg(pil_img):\n",
        "    img = np.array(pil_img).astype(np.float32)/255.0\n",
        "    img = tf.image.resize(img,(512,512))\n",
        "    return img[tf.newaxis,:]\n",
        "\n",
        "\n",
        "# Blend multiple style targets\n",
        "def blend_style_targets(style_images, extractor, blend_weights):\n",
        "    blended = None\n",
        "    for i,img in enumerate(style_images):\n",
        "        target = extractor(preprocess_img_vgg(img))['style']\n",
        "        # Initialize with weighted first style\n",
        "        if blended is None:\n",
        "            blended = {k:v*blend_weights[i] for k,v in target.items()}\n",
        "        # Add weighted styles to existing dict\n",
        "        else:\n",
        "            [blended.update({k:blended[k]+target[k]*blend_weights[i]}) for k in blended]\n",
        "    return blended\n",
        "\n",
        "\n",
        "# Optimization loop for VGG-based NST\n",
        "def run_style_transfer_vgg(content, style_imgs, blend_weights, alpha, beta):\n",
        "    content_in = preprocess_img_vgg(content)\n",
        "    extractor = VGGStyleContentModel(\n",
        "        # style layers\n",
        "        ['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1'],\n",
        "        # content layer\n",
        "        ['block5_conv2'])\n",
        "    targets = {\n",
        "        'style': blend_style_targets(style_imgs, extractor, blend_weights),\n",
        "        'content': extractor(content_in)['content']\n",
        "    }\n",
        "\n",
        "    # Start with content image as variable\n",
        "    img = tf.Variable(content_in)\n",
        "    # Adam optimizer\n",
        "    opt = tf.optimizers.Adam(0.02)\n",
        "\n",
        "    # 50 iterations optimization\n",
        "    for _ in range(50):\n",
        "        with tf.GradientTape() as tape:\n",
        "            out = extractor(img)\n",
        "            s_loss = tf.add_n([tf.reduce_mean((out['style'][k]-targets['style'][k])**2) for k in targets['style']])\n",
        "            c_loss = tf.add_n([tf.reduce_mean((out['content'][k]-targets['content'][k])**2) for k in targets['content']])\n",
        "            loss = alpha*c_loss + beta*s_loss\n",
        "        # Compute gradients\n",
        "        grad = tape.gradient(loss, img)\n",
        "        # Apply update\n",
        "        opt.apply_gradients([(grad,img)])\n",
        "        # Keep valid range\n",
        "        img.assign(tf.clip_by_value(img,0,1))\n",
        "    return tf.squeeze(img).numpy()\n",
        "\n",
        "\n",
        "# Gradio callback for VGG optimizer stylization\n",
        "def stylize_ui_vgg(content,s1,s2,s3,s4,s5,\n",
        "                   w1,w2,w3,w4,w5,\n",
        "                   alpha,beta,preserve_colour,foreground_only,sharpness):\n",
        "    # Collect style images and weights\n",
        "    style_imgs,blend_weights = collect_styles_and_weights([s1,s2,s3,s4,s5],[w1,w2,w3,w4,w5])\n",
        "    # Run optimization\n",
        "    out = run_style_transfer_vgg(content, style_imgs, blend_weights, alpha, beta)\n",
        "    # Resize content for extras\n",
        "    content_resized = np.array(content.resize((512,512))).astype(np.float32)/255.0\n",
        "\n",
        "    # Apply optional extras\n",
        "    if preserve_colour:\n",
        "        out = apply_colour_preservation(out, content_resized)\n",
        "    if sharpness>0:\n",
        "        out = apply_sharpness(out, sharpness)\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        out = mask*out + (1.0-mask)*content_resized\n",
        "    return np.clip(out,0,1)\n",
        "\n",
        "# MobileNet Optimizer (Multi-stage)\n",
        "\n",
        "# Define the style layers we want to extract from MobileNetV2\n",
        "STYLE_LAYERS = [\n",
        "    'block_1_expand_relu','block_3_expand_relu','block_6_expand_relu',\n",
        "    'block_10_expand_relu','block_13_expand_relu'\n",
        "]\n",
        "\n",
        "# Assign different weights to each style layer (lower layers capture fine textures, deeper layers capture structure)\n",
        "STYLE_LAYER_WEIGHTS = {\n",
        "    # Early layer has the highest weight\n",
        "    'block_1_expand_relu':1.0,\n",
        "    'block_3_expand_relu':0.8,\n",
        "    'block_6_expand_relu':0.6,\n",
        "    'block_10_expand_relu':0.5,\n",
        "    # Deepest style layer has the lowest weight\n",
        "    'block_13_expand_relu':0.4\n",
        "}\n",
        "\n",
        "# Define the content layers (deep layers preserve high-level structure)\n",
        "CONTENT_LAYERS = ['block_13_expand_relu','block_16_project']\n",
        "\n",
        "\n",
        "# MobileNet feature extractor class\n",
        "class MNetStyleContentModel(tf.keras.models.Model):\n",
        "    def __init__(self, style_layers, content_layers, img_size):\n",
        "        super().__init__()\n",
        "        # Store style and content layer names\n",
        "        self.style_layers = style_layers\n",
        "        self.content_layers = content_layers\n",
        "        self.num_style = len(style_layers)\n",
        "\n",
        "        # Load pretrained MobileNetV2\n",
        "        base = mobilenet_v2.MobileNetV2(include_top=False,\n",
        "                                        weights='imagenet',\n",
        "                                        input_shape=(img_size,img_size,3))\n",
        "        base.trainable = False\n",
        "\n",
        "        # Collect outputs from specified style and content layers\n",
        "        outs = [base.get_layer(n).output for n in style_layers+content_layers]\n",
        "\n",
        "        # Build encoder model that outputs these intermediate activations\n",
        "        self.encoder = Model([base.input], outs)\n",
        "\n",
        "    def call(self, x):\n",
        "        # Pass input image through MobileNet encoder\n",
        "        outs = self.encoder(x)\n",
        "\n",
        "        # Compute Gram matrices for style outputs (first part of outs)\n",
        "        style_outs = [gram_matrix(o) for o in outs[:self.num_style]]\n",
        "\n",
        "        # Return dictionary with style and content features\n",
        "        return {\n",
        "            'style': {n:v for n,v in zip(self.style_layers, style_outs)},\n",
        "            'content': {n:v for n,v in zip(self.content_layers, outs[self.num_style:])}\n",
        "        }\n",
        "\n",
        "\n",
        "# Preprocess input PIL image for MobileNetV2\n",
        "def preprocess_for_model(pil_img, img_size):\n",
        "   # Convert PIL → numpy [0,1]\n",
        "    x = np.array(pil_img).astype(np.float32)/255.0\n",
        "    # Resize to target size\n",
        "    x = tf.image.resize(x, (img_size,img_size), antialias=True)\n",
        "    # Apply MobileNet-specific preprocessing\n",
        "    x = mobilenet_v2.preprocess_input(x*255.0)\n",
        "    return x[None,...]\n",
        "\n",
        "\n",
        "# Convert MobileNet output ([-1,1]) back to [0,1] float image\n",
        "def deprocess_from_model(x_minus1_to1):\n",
        "    # Remove batch dimension if present\n",
        "    x = tf.squeeze(x_minus1_to1,0)\n",
        "    x = tf.clip_by_value((x+1.0)/2.0, 0.0, 1.0)\n",
        "    return x.numpy().astype(np.float32)\n",
        "\n",
        "\n",
        "# Blend multiple style targets for MobileNet\n",
        "def blended_style_targets(extractor, style_pils, blend_weights, img_size):\n",
        "    accum = None\n",
        "    for img, w in zip(style_pils, blend_weights):\n",
        "        # Extract style features for current image\n",
        "        st = extractor(preprocess_for_model(img, img_size))['style']\n",
        "        # First style image to initialise accumulator\n",
        "        if accum is None:\n",
        "            accum = {k: v*w for k,v in st.items()}\n",
        "        # Add weighted style features to accumulator\n",
        "        else:\n",
        "            [accum.update({k: accum[k] + st[k]*w}) for k in accum]\n",
        "\n",
        "    # Return weighted blended style targets\n",
        "    return accum\n",
        "\n",
        "\n",
        "# Laplacian operator for edge loss\n",
        "\n",
        "def laplacian(img_minus1_to1):\n",
        "    # Convert input image from [-1, 1] range to [0, 1]\n",
        "    x = (img_minus1_to1 + 1.0) / 2.0\n",
        "\n",
        "    # Define Laplacian kernel (edge detector)\n",
        "    k = tf.constant([[0., -1., 0.],\n",
        "                     [-1., 4., -1.],\n",
        "                     [0., -1., 0.]], tf.float32)\n",
        "\n",
        "    # Reshape kernel to 4D shape [H,W,in_channels,out_channels]\n",
        "    k = tf.reshape(k, [3, 3, 1, 1])\n",
        "\n",
        "    # Repeat kernel for all 3 RGB channels\n",
        "    k = tf.repeat(k, 3, axis=2)\n",
        "\n",
        "    # Apply convolution (SAME padding keeps size)\n",
        "    return tf.nn.conv2d(x, k, strides=1, padding='SAME')\n",
        "\n",
        "\n",
        "# Multi-stage optimization loop (224 → 512 → 768 )\n",
        "\n",
        "def run_stage(content_pil, style_pils, blend_weights, img_size,\n",
        "              steps, lr_start, lr_end, alpha_content, beta_style,\n",
        "              tv_weight=1e-6, edge_weight=5e-3, init_from=None):\n",
        "\n",
        "    # Preprocess the content image to MobileNet format at target resolution\n",
        "    content = preprocess_for_model(content_pil, img_size)\n",
        "\n",
        "    # Initialize MobileNet-based extractor with style/content layers\n",
        "    extractor = MNetStyleContentModel(STYLE_LAYERS, CONTENT_LAYERS, img_size)\n",
        "\n",
        "    # Get blended style features from all style images\n",
        "    style_tgt = blended_style_targets(extractor, style_pils, blend_weights, img_size)\n",
        "\n",
        "    # Get content target features\n",
        "    content_tgt = extractor(content)['content']\n",
        "\n",
        "    # If no initialization image is passed\n",
        "    if init_from is None:\n",
        "        # Use weighted mix of content and first style image as seed\n",
        "        style_seed = preprocess_for_model(style_pils[0], img_size)\n",
        "        seed = tf.clip_by_value(((content + 1) / 2.0) * 0.6 + ((style_seed + 1) / 2.0) * 0.4, 0, 1)\n",
        "        image = tf.Variable(mobilenet_v2.preprocess_input(seed * 255.0))\n",
        "    else:\n",
        "        # Upscale previous stage output to current size and use it as init\n",
        "        up = tf.image.resize(init_from, (img_size, img_size), method='bicubic', antialias=True)\n",
        "        image = tf.Variable(up)\n",
        "    lr_schedule = tf.keras.optimizers.schedules.CosineDecay(lr_start, steps, alpha=lr_end / lr_start)\n",
        "    opt = tf.optimizers.Adam(lr_schedule)\n",
        "\n",
        "    # Normalize style loss by total style weights\n",
        "    denom = sum(STYLE_LAYER_WEIGHTS.values())\n",
        "\n",
        "    # Define one training step\n",
        "    @tf.function\n",
        "    def train_step(img):\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Extract features from current image\n",
        "            outs = extractor(img)\n",
        "            s, c = outs['style'], outs['content']\n",
        "\n",
        "            # Compute style loss (weighted MSE across layers)\n",
        "            s_loss = tf.add_n([\n",
        "                STYLE_LAYER_WEIGHTS[k] * tf.reduce_mean((s[k] - style_tgt[k]) ** 2)\n",
        "                for k in s\n",
        "            ]) * (beta_style / denom)\n",
        "\n",
        "            # Compute content loss (MSE across layers)\n",
        "            c_loss = tf.add_n([\n",
        "                tf.reduce_mean((c[k] - content_tgt[k]) ** 2)\n",
        "                for k in c\n",
        "            ]) * (alpha_content / len(CONTENT_LAYERS))\n",
        "\n",
        "            # Add total variation loss (encourages smoothness)\n",
        "            tv = tf.reduce_mean(tf.image.total_variation(img)) * tv_weight\n",
        "\n",
        "            # Add edge loss (match Laplacian edges of content and stylized)\n",
        "            edge = tf.reduce_mean((laplacian(img) - laplacian(content)) ** 2) * edge_weight\n",
        "\n",
        "            # Final total loss\n",
        "            loss = s_loss + c_loss + tv + edge\n",
        "\n",
        "        # Compute gradients of loss wrt. image\n",
        "        grad = tape.gradient(loss, img)\n",
        "\n",
        "        # Clip gradients to avoid exploding updates\n",
        "        grad = tf.clip_by_norm(grad, 10.0)\n",
        "\n",
        "        # Apply optimizer step\n",
        "        opt.apply_gradients([(grad, img)])\n",
        "\n",
        "        # Clamp image values back into valid range [-1, 1]\n",
        "        img.assign(tf.clip_by_value(img, -1, 1))\n",
        "        return loss\n",
        "\n",
        "    # Run training loop for the specified number of steps\n",
        "    for _ in range(steps):\n",
        "        _ = train_step(image)\n",
        "\n",
        "    # Return optimized image tensor\n",
        "    return image\n",
        "\n",
        "# Gradio callback for MobileNet style transfer run\n",
        "\n",
        "def stylize_ui_mnet(content, s1, s2, s3, s4, s5,\n",
        "                    w1, w2, w3, w4, w5,\n",
        "                    alpha, beta, preserve_colour, foreground_only, sharpness):\n",
        "\n",
        "    # Collect style images and normalize weights\n",
        "    style_imgs, blend_weights = collect_styles_and_weights(\n",
        "        [s1, s2, s3, s4, s5],\n",
        "        [w1, w2, w3, w4, w5]\n",
        "    )\n",
        "\n",
        "    # Stage 1: Optimize at 224x224 resolution\n",
        "    out_224 = run_stage(content, style_imgs, blend_weights, 224, 400, 0.05, 0.01, alpha, beta)\n",
        "    # Stage 2: Optimize at 512x512 resolution, initialized from stage 1\n",
        "    out_512 = run_stage(content, style_imgs, blend_weights, 512, 350, 0.03, 0.006, alpha * 2, beta, init_from=out_224)\n",
        "    # Stage 3: Optimize at 768x768 resolution, initialized from stage 2\n",
        "    out_768 = run_stage(content, style_imgs, blend_weights, 768, 300, 0.02, 0.004, alpha * 3, beta, init_from=out_512)\n",
        "\n",
        "    # Convert final output tensor back to [0,1] float image\n",
        "    stylized = deprocess_from_model(out_768)\n",
        "\n",
        "    # Resize final result to 512x512 for UI consistency\n",
        "    stylized = tf.image.resize(stylized, (512, 512), antialias=True, method='bicubic').numpy()\n",
        "\n",
        "    # Convert content image to float [0,1] numpy\n",
        "    content_np = to_float01(np.array(content))\n",
        "\n",
        "    # Resize content to 512x512 for post-processing steps\n",
        "    content_resized = np.array(\n",
        "        Image.fromarray((content_np * 255).astype(np.uint8)).resize((512, 512), Image.BILINEAR)\n",
        "    ).astype(np.float32) / 255.0\n",
        "\n",
        "    # If user enabled colour preservation\n",
        "    if preserve_colour:\n",
        "        # Retain colours from the original content image and combine with the textures from the stylised image\n",
        "        stylized = apply_colour_preservation(stylized, content_resized)\n",
        "\n",
        "    # If user enabled sharpness, apply unsharp mask filter\n",
        "    if sharpness > 0:\n",
        "        stylized = apply_sharpness(stylized, sharpness)\n",
        "\n",
        "    # If user enabled foreground-only styling, mask out background\n",
        "    if foreground_only:\n",
        "        mask = make_black_mask_from_np(content_resized, threshold=0.05)\n",
        "        stylized = mask * stylized + (1.0 - mask) * content_resized\n",
        "\n",
        "    # Return final image clipped into [0,1]\n",
        "    return np.clip(stylized, 0, 1)\n",
        "\n",
        "\n",
        "# Dispatch routes request to chosen NST model\n",
        "def unified_stylize(model_choice, content,\n",
        "                    s1, s2, s3, s4, s5,\n",
        "                    w1, w2, w3, w4, w5,\n",
        "                    alpha, beta, preserve_colour, foreground_only, sharpness):\n",
        "\n",
        "    # Run AdaIN model if selected\n",
        "    if model_choice == \"AdaIN\":\n",
        "        out = stylize_ui_adain(content, s1, s2, s3, s4, s5,\n",
        "                               w1, w2, w3, w4, w5,\n",
        "                               alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Run VGG optimizer if selected\n",
        "    elif model_choice == \"VGG Optimizer\":\n",
        "        out = stylize_ui_vgg(content, s1, s2, s3, s4, s5,\n",
        "                             w1, w2, w3, w4, w5,\n",
        "                             alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Run MobileNet optimizer if selected\n",
        "    elif model_choice == \"MobileNet Optimizer\":\n",
        "        out = stylize_ui_mnet(content, s1, s2, s3, s4, s5,\n",
        "                              w1, w2, w3, w4, w5,\n",
        "                              alpha, beta, preserve_colour, foreground_only, sharpness)\n",
        "\n",
        "    # Compute SSIM and LPIPS metrics for evaluation\n",
        "    style_imgs, blend_weights = collect_styles_and_weights([s1, s2, s3, s4, s5],\n",
        "                                                      [w1, w2, w3, w4, w5])\n",
        "    ssim_val, lpips_c, lpips_s = compute_metrics(out, content, style_imgs, blend_weights)\n",
        "\n",
        "    # Return stylised image and metrics\n",
        "    return out, ssim_val, lpips_c, lpips_s\n",
        "\n",
        "# Gradio User Interface\n",
        "with gr.Blocks() as demo:\n",
        "\n",
        "    # Title and description\n",
        "    # Higher SSIM , better performance\n",
        "    # Lower LPIPS , better performance\n",
        "    gr.Markdown(\"##  3-in-1 NST App : AdaIN | VGG Optimizer | MobileNet Optimizer\")\n",
        "    gr.Markdown(\n",
        "        \"After you click \\\"Stylise\\\" — your chosen model makes the stylised image, \"\n",
        "        \"then metrics are computed:\\n\"\n",
        "    )\n",
        "\n",
        "    # Create dropdown menu for model selection\n",
        "    model_choice = gr.Dropdown(\n",
        "        [\"AdaIN\", \"VGG Optimizer\", \"MobileNet Optimizer\"],\n",
        "        value=\"AdaIN\",\n",
        "        label=\"Select Model\"\n",
        "    )\n",
        "\n",
        "    # Upload option for content image\n",
        "    content_input = gr.Image(\n",
        "        label=\"Content Image\",\n",
        "        type=\"pil\",\n",
        "        image_mode=\"RGB\",\n",
        "        height=256,\n",
        "        width=256\n",
        "    )\n",
        "\n",
        "    # Row of 5 style image upload option facilities\n",
        "    with gr.Row():\n",
        "        style_input1 = gr.Image(label=\"Style 1\", type=\"pil\")\n",
        "        style_input2 = gr.Image(label=\"Style 2\", type=\"pil\")\n",
        "        style_input3 = gr.Image(label=\"Style 3\", type=\"pil\")\n",
        "        style_input4 = gr.Image(label=\"Style 4\", type=\"pil\")\n",
        "        style_input5 = gr.Image(label=\"Style 5\", type=\"pil\")\n",
        "\n",
        "    # Initialise Sliders to adjust style weights of 5 style images\n",
        "    with gr.Row():\n",
        "        blend1 = gr.Slider(0, 1, 1, step=0.01, label=\"Style 1 Strength\")\n",
        "        blend2 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 2 Strength\")\n",
        "        blend3 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 3 Strength\")\n",
        "        blend4 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 4 Strength\")\n",
        "        blend5 = gr.Slider(0, 1, 0, step=0.01, label=\"Style 5 Strength\")\n",
        "\n",
        "    # Controls for content/style weights , colour preservation , foreground styling and sharpness controls\n",
        "    with gr.Row():\n",
        "        alpha_slider = gr.Slider(1, 5000, 1000, step=10, label=\"α (Content Weight)\")\n",
        "        beta_slider = gr.Slider(1e-3, 1.0, 1e-2, step=1e-3, label=\"β (Style Weight)\")\n",
        "        preserve_colour = gr.Checkbox(label=\"Apply Colour Preservation\", value=False)\n",
        "        foreground_only = gr.Checkbox(label=\"Apply Foreground-Aware Styling\", value=False)\n",
        "        sharpness = gr.Slider(0, 1, 0.5, step=0.05, label=\"Style Sharpness\")\n",
        "\n",
        "    # Run button\n",
        "    run_btn = gr.Button(\"Stylise\")\n",
        "\n",
        "    # Output row to display final image and metrics\n",
        "    with gr.Row():\n",
        "        output_img = gr.Image(label=\"Stylised Output\", image_mode=\"RGB\", height=512, width=512)\n",
        "        with gr.Column():\n",
        "            ssim_metric = gr.Number(label=\"SSIM vs Content (↑ better)\")\n",
        "            lpips_content_metric = gr.Number(label=\"LPIPS vs Content (↓ better)\")\n",
        "            lpips_style_metric = gr.Number(label=\"LPIPS vs Blended Style (↓ better)\")\n",
        "\n",
        "    # Connect button to unified stylisation function\n",
        "    run_btn.click(\n",
        "        fn=unified_stylize,\n",
        "        inputs=[model_choice, content_input,\n",
        "                style_input1, style_input2, style_input3, style_input4, style_input5,\n",
        "                blend1, blend2, blend3, blend4, blend5,\n",
        "                alpha_slider, beta_slider, preserve_colour, foreground_only, sharpness],\n",
        "        outputs=[output_img, ssim_metric, lpips_content_metric, lpips_style_metric]\n",
        "    )\n",
        "\n",
        "# Launch app\n",
        "demo.launch()\n"
      ]
    }
  ]
}